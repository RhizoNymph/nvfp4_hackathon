[3313] python3.10@127.0.0.1
  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,982,062,780.27
    SM Frequency                        1,116,510,183.11
    Elapsed Cycles                                24,096
    Memory Throughput                              11.90
    DRAM Throughput                                11.90
    Duration                                      21,408
    L1/TEX Cache Throughput                        40.22
    L2 Cache Throughput                            11.64
    SM Active Cycles                            4,093.76
    Compute (SM) Throughput                        10.28
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.50
    Executed Ipc Elapsed                     0.09
    Issue Slots Busy                        12.40
    Issued Ipc Active                        0.50
    SM Busy                                 57.97
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (59.4%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        790,768,310,911.81
    Mem Busy                                              10.35
    Max Bandwidth                                         11.90
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           24.21
    Mem Pipes Busy                                        10.28
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.511%                                                                                     
          Out of the 682336.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To         
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 14.74%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 3.2 - way bank        
          conflict across all 480 shared load requests.This results in 555 bank conflicts,  which represent 36.63% of   
          the overall 1515 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.41
    Issued Warp Per Scheduler                        0.12
    No Eligible                                     87.59
    Active Warps Per Scheduler                       1.66
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.59%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.66 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.41
    Warp Cycles Per Executed Instruction                        13.41
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 62.17%                                                                                     
          On average, each warp of this workload spends 8.3 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.2% of the total average of 13.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7404%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.40
    Achieved Active Warps Per SM                        6.66
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.5%                                                                                      
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,266
    Total DRAM Elapsed Cycles                       5,455,872
    Average L1 Active Cycles                         4,093.76
    Total L1 Elapsed Cycles                         3,499,424
    Average L2 Active Cycles                        14,683.17
    Total L2 Elapsed Cycles                         4,544,616
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,093.76
    Total SM Elapsed Cycles                         3,499,424
    Average SMSP Active Cycles                       4,092.12
    Total SMSP Elapsed Cycles                      13,997,696
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.67%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.94% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.65%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.86% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.67%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.94% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,971
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,981,782,945.74
    SM Frequency                        1,112,590,843.02
    Elapsed Cycles                                23,224
    Memory Throughput                              12.62
    DRAM Throughput                                12.62
    Duration                                      20,640
    L1/TEX Cache Throughput                        40.68
    L2 Cache Throughput                            12.35
    SM Active Cycles                            4,106.32
    Compute (SM) Throughput                         9.88
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.49
    Executed Ipc Elapsed                     0.08
    Issue Slots Busy                        12.36
    Issued Ipc Active                        0.49
    SM Busy                                 57.73
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (59.2%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        839,627,906,976.74
    Mem Busy                                              11.18
    Max Bandwidth                                         12.62
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           24.27
    Mem Pipes Busy                                         9.88
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.893%                                                                                     
          Out of the 1465824.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To        
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 14.45%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 3.1 - way bank        
          conflict across all 480 shared load requests.This results in 529 bank conflicts,  which represent 35.53% of   
          the overall 1489 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.48
    Issued Warp Per Scheduler                        0.12
    No Eligible                                     87.52
    Active Warps Per Scheduler                       1.67
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.38%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.67 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.35
    Warp Cycles Per Executed Instruction                        13.35
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 60.58%                                                                                     
          On average, each warp of this workload spends 8.1 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.6% of the total average of 13.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7381%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.33
    Achieved Active Warps Per SM                        6.61
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.38%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,461.88
    Total DRAM Elapsed Cycles                       5,259,776
    Average L1 Active Cycles                         4,106.32
    Total L1 Elapsed Cycles                         3,641,704
    Average L2 Active Cycles                        16,590.01
    Total L2 Elapsed Cycles                         4,385,640
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,106.32
    Total SM Elapsed Cycles                         3,641,704
    Average SMSP Active Cycles                       4,068.40
    Total SMSP Elapsed Cycles                      14,566,816
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.13%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.68% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.03%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.83% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.13%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.68% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,982,087,227.41
    SM Frequency                        1,120,065,469.24
    Elapsed Cycles                                23,212
    Memory Throughput                              12.51
    DRAM Throughput                                12.51
    Duration                                      20,544
    L1/TEX Cache Throughput                        40.24
    L2 Cache Throughput                            12.24
    SM Active Cycles                            4,092.57
    Compute (SM) Throughput                        10.42
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.50
    Executed Ipc Elapsed                     0.09
    Issue Slots Busy                        12.40
    Issued Ipc Active                        0.50
    SM Busy                                 57.78
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (59.4%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        832,386,292,834.89
    Mem Busy                                              11.04
    Max Bandwidth                                         12.51
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           23.58
    Mem Pipes Busy                                        10.42
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.913%                                                                                     
          Out of the 1071040.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To        
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 19.33%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 3.9 - way bank        
          conflict across all 480 shared load requests.This results in 888 bank conflicts,  which represent 48.05% of   
          the overall 1848 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.39
    Issued Warp Per Scheduler                        0.12
    No Eligible                                     87.61
    Active Warps Per Scheduler                       1.67
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.49%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.67 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.45
    Warp Cycles Per Executed Instruction                        13.45
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 60.93%                                                                                     
          On average, each warp of this workload spends 8.2 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.9% of the total average of 13.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7406%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.44
    Achieved Active Warps Per SM                        6.68
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.49%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,349.88
    Total DRAM Elapsed Cycles                       5,235,712
    Average L1 Active Cycles                         4,092.57
    Total L1 Elapsed Cycles                         3,454,524
    Average L2 Active Cycles                        15,263.70
    Total L2 Elapsed Cycles                         4,367,148
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,092.57
    Total SM Elapsed Cycles                         3,454,524
    Average SMSP Active Cycles                       4,096.07
    Total SMSP Elapsed Cycles                      13,818,096
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.79%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.63% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.88%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.79%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.63% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,987,915,407.85
    SM Frequency                        1,119,771,997.73
    Elapsed Cycles                                23,886
    Memory Throughput                              11.99
    DRAM Throughput                                11.99
    Duration                                      21,184
    L1/TEX Cache Throughput                        40.21
    L2 Cache Throughput                            11.72
    SM Active Cycles                            4,094.89
    Compute (SM) Throughput                        10.07
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.50
    Executed Ipc Elapsed                     0.08
    Issue Slots Busy                        12.40
    Issued Ipc Active                        0.50
    SM Busy                                 58.51
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (59.4%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        797,776,435,045.32
    Mem Busy                                              10.53
    Max Bandwidth                                         11.99
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           25.68
    Mem Pipes Busy                                        10.07
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.6%                                                                                       
          Out of the 610176.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To         
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 15.29%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 3.2 - way bank        
          conflict across all 480 shared load requests.This results in 589 bank conflicts,  which represent 38.02% of   
          the overall 1549 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.63
    Issued Warp Per Scheduler                        0.13
    No Eligible                                     87.37
    Active Warps Per Scheduler                       1.68
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.37%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 7.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.68 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.29
    Warp Cycles Per Executed Instruction                        13.29
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 61.49%                                                                                     
          On average, each warp of this workload spends 8.2 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.5% of the total average of 13.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7402%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.31
    Achieved Active Warps Per SM                        6.60
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.37%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,252
    Total DRAM Elapsed Cycles                       5,406,720
    Average L1 Active Cycles                         4,094.89
    Total L1 Elapsed Cycles                         3,571,802
    Average L2 Active Cycles                        15,157.45
    Total L2 Elapsed Cycles                         4,504,412
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,094.89
    Total SM Elapsed Cycles                         3,571,802
    Average SMSP Active Cycles                       4,019.13
    Total SMSP Elapsed Cycles                      14,287,208
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.33%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.54% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.14%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.88% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.33%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.54% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,976,973,684.21
    SM Frequency                        1,116,461,880.80
    Elapsed Cycles                                23,284
    Memory Throughput                              12.28
    DRAM Throughput                                12.28
    Duration                                      20,672
    L1/TEX Cache Throughput                        40.24
    L2 Cache Throughput                            12.01
    SM Active Cycles                            4,092.28
    Compute (SM) Throughput                         9.96
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.50
    Executed Ipc Elapsed                     0.08
    Issue Slots Busy                        12.41
    Issued Ipc Active                        0.50
    SM Busy                                 57.93
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (59.5%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        816,148,606,811.15
    Mem Busy                                              10.79
    Max Bandwidth                                         12.28
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           24.44
    Mem Pipes Busy                                         9.96
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.866%                                                                                     
          Out of the 546496.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To         
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 14.55%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 3.1 - way bank        
          conflict across all 480 shared load requests.This results in 544 bank conflicts,  which represent 36.17% of   
          the overall 1504 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.27
    Issued Warp Per Scheduler                        0.12
    No Eligible                                     87.73
    Active Warps Per Scheduler                       1.63
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.72%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.63 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.28
    Warp Cycles Per Executed Instruction                        13.28
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 61.67%                                                                                     
          On average, each warp of this workload spends 8.2 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.7% of the total average of 13.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7407%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.31
    Achieved Active Warps Per SM                        6.60
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.5%                                                                                      
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,238
    Total DRAM Elapsed Cycles                       5,261,568
    Average L1 Active Cycles                         4,092.28
    Total L1 Elapsed Cycles                         3,616,098
    Average L2 Active Cycles                        14,630.61
    Total L2 Elapsed Cycles                         4,389,780
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,092.28
    Total SM Elapsed Cycles                         3,616,098
    Average SMSP Active Cycles                       4,135.63
    Total SMSP Elapsed Cycles                      14,464,392
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.24%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.02% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.39%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.12% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.24%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.02% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 6.142%                                                                                     
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 10.02% above the average, while the minimum instance value is 6.07% below the       
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (4, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,980,932,203.39
    SM Frequency                        1,085,325,079.45
    Elapsed Cycles                                25,152
    Memory Throughput                              15.73
    DRAM Throughput                                11.67
    Duration                                      22,656
    L1/TEX Cache Throughput                        39.91
    L2 Cache Throughput                            16.61
    SM Active Cycles                            8,252.44
    Compute (SM) Throughput                        19.80
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.4 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.49
    Executed Ipc Elapsed                     0.17
    Issue Slots Busy                        12.30
    Issued Ipc Active                        0.49
    SM Busy                                 57.38
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (58.9%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        776,316,384,180.79
    Mem Busy                                              15.63
    Max Bandwidth                                         15.73
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           48.29
    Mem Pipes Busy                                        19.80
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.235%                                                                                     
          Out of the 2056000.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To        
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 13.93%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 3.1 - way bank        
          conflict across all 960 shared load requests.This results in 1029 bank conflicts,  which represent 34.89% of  
          the overall 2949 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.44
    Issued Warp Per Scheduler                        0.12
    No Eligible                                     87.56
    Active Warps Per Scheduler                       1.67
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.2%                                                                                      
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.67 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.41
    Warp Cycles Per Executed Instruction                        13.41
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 61.16%                                                                                     
          On average, each warp of this workload spends 8.2 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.2% of the total average of 13.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                 1,015.19
    Executed Instructions                                     600,992
    Avg. Issued Instructions Per Scheduler                   1,015.30
    Issued Instructions                                       601,056
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7346%                                                                                    
          This kernel executes 0 fused and 1664 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 64
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                               16,384
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.43
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 56.76%                                                                                     
          The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.32
    Achieved Active Warps Per SM                        6.61
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.2%                                                                                      
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,588
    Total DRAM Elapsed Cycles                       5,772,288
    Average L1 Active Cycles                         8,252.44
    Total L1 Elapsed Cycles                         3,633,048
    Average L2 Active Cycles                        17,574.82
    Total L2 Elapsed Cycles                         4,814,452
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         8,252.44
    Total SM Elapsed Cycles                         3,633,048
    Average SMSP Active Cycles                       8,163.90
    Total SMSP Elapsed Cycles                      14,532,192
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 19.61%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 58.33% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 19.52%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 58.68% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 19.61%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 58.33% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         39,936
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       4.05
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,979,229,607.25
    SM Frequency                        1,116,803,955.82
    Elapsed Cycles                                23,915
    Memory Throughput                              12.06
    DRAM Throughput                                12.06
    Duration                                      21,184
    L1/TEX Cache Throughput                        39.98
    L2 Cache Throughput                            11.80
    SM Active Cycles                            4,119.08
    Compute (SM) Throughput                         9.90
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.49
    Executed Ipc Elapsed                     0.08
    Issue Slots Busy                        12.32
    Issued Ipc Active                        0.49
    SM Busy                                 57.53
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (59.0%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        801,836,858,006.04
    Mem Busy                                              10.75
    Max Bandwidth                                         12.06
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           22.21
    Mem Pipes Busy                                         9.90
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.619%                                                                                     
          Out of the 807104.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To         
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 14.59%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 3.1 - way bank        
          conflict across all 480 shared load requests.This results in 552 bank conflicts,  which represent 36.51% of   
          the overall 1512 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.58
    Issued Warp Per Scheduler                        0.13
    No Eligible                                     87.42
    Active Warps Per Scheduler                       1.68
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.42%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 7.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.68 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.31
    Warp Cycles Per Executed Instruction                        13.32
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 61.54%                                                                                     
          On average, each warp of this workload spends 8.2 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.5% of the total average of 13.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7359%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.27
    Achieved Active Warps Per SM                        6.57
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.42%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,294
    Total DRAM Elapsed Cycles                       5,394,944
    Average L1 Active Cycles                         4,119.08
    Total L1 Elapsed Cycles                         3,636,372
    Average L2 Active Cycles                        15,036.62
    Total L2 Elapsed Cycles                         4,499,076
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,119.08
    Total SM Elapsed Cycles                         3,636,372
    Average SMSP Active Cycles                       4,034.47
    Total SMSP Elapsed Cycles                      14,545,488
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.21%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.78% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 12.98%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.05% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.21%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.78% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,975,961,538.46
    SM Frequency                        1,110,853,365.38
    Elapsed Cycles                                23,432
    Memory Throughput                              12.50
    DRAM Throughput                                12.50
    Duration                                      20,800
    L1/TEX Cache Throughput                        40.48
    L2 Cache Throughput                            12.23
    SM Active Cycles                            4,068.36
    Compute (SM) Throughput                        10.34
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.50
    Executed Ipc Elapsed                     0.09
    Issue Slots Busy                        12.48
    Issued Ipc Active                        0.50
    SM Busy                                 58.23
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (59.9%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        830,412,307,692.31
    Mem Busy                                              11.24
    Max Bandwidth                                         12.50
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           24.47
    Mem Pipes Busy                                        10.34
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.835%                                                                                     
          Out of the 1334752.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To        
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 13.28%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 3.0 - way bank        
          conflict across all 480 shared load requests.This results in 469 bank conflicts,  which represent 32.82% of   
          the overall 1429 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.40
    Issued Warp Per Scheduler                        0.12
    No Eligible                                     87.60
    Active Warps Per Scheduler                       1.66
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.5%                                                                                      
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.66 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.40
    Warp Cycles Per Executed Instruction                        13.40
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 61.93%                                                                                     
          On average, each warp of this workload spends 8.3 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.9% of the total average of 13.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.745%                                                                                     
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.46
    Achieved Active Warps Per SM                        6.70
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.5%                                                                                      
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,433.88
    Total DRAM Elapsed Cycles                       5,292,800
    Average L1 Active Cycles                         4,068.36
    Total L1 Elapsed Cycles                         3,487,724
    Average L2 Active Cycles                        15,805.77
    Total L2 Elapsed Cycles                         4,414,344
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,068.36
    Total SM Elapsed Cycles                         3,487,724
    Average SMSP Active Cycles                       4,093.20
    Total SMSP Elapsed Cycles                      13,950,896
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.61%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.85% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.7%                                                                                      
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.90% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.61%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.85% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,976,371,951.22
    SM Frequency                        1,122,725,323.93
    Elapsed Cycles                                23,731
    Memory Throughput                              12.14
    DRAM Throughput                                12.14
    Duration                                      20,992
    L1/TEX Cache Throughput                        40.24
    L2 Cache Throughput                            11.87
    SM Active Cycles                            4,092.42
    Compute (SM) Throughput                        10.04
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.50
    Executed Ipc Elapsed                     0.08
    Issue Slots Busy                        12.40
    Issued Ipc Active                        0.50
    SM Busy                                 57.93
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (59.5%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        806,439,024,390.24
    Mem Busy                                              10.68
    Max Bandwidth                                         12.14
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           23.61
    Mem Pipes Busy                                        10.04
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.714%                                                                                     
          Out of the 675744.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To         
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 8.549%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 2.5 - way bank        
          conflict across all 480 shared load requests.This results in 259 bank conflicts,  which represent 21.25% of   
          the overall 1219 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.56
    Issued Warp Per Scheduler                        0.13
    No Eligible                                     87.44
    Active Warps Per Scheduler                       1.68
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.44%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.68 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.41
    Warp Cycles Per Executed Instruction                        13.41
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 61.57%                                                                                     
          On average, each warp of this workload spends 8.3 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.6% of the total average of 13.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7407%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.41
    Achieved Active Warps Per SM                        6.66
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.44%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,266
    Total DRAM Elapsed Cycles                       5,342,208
    Average L1 Active Cycles                         4,092.42
    Total L1 Elapsed Cycles                         3,588,618
    Average L2 Active Cycles                        14,910.33
    Total L2 Elapsed Cycles                         4,455,560
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,092.42
    Total SM Elapsed Cycles                         3,588,618
    Average SMSP Active Cycles                       4,040.62
    Total SMSP Elapsed Cycles                      14,354,472
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.29%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.75% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.14%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.88% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.29%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.75% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,978,735,207.10
    SM Frequency                        1,109,692,816.20
    Elapsed Cycles                                24,436
    Memory Throughput                              12.04
    DRAM Throughput                                12.04
    Duration                                      21,632
    L1/TEX Cache Throughput                        40.74
    L2 Cache Throughput                            11.78
    SM Active Cycles                            4,102.86
    Compute (SM) Throughput                        10.31
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.49
    Executed Ipc Elapsed                     0.09
    Issue Slots Busy                        12.37
    Issued Ipc Active                        0.49
    SM Busy                                 57.85
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (59.3%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        801,124,260,355.03
    Mem Busy                                              10.86
    Max Bandwidth                                         12.04
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           23.03
    Mem Pipes Busy                                        10.31
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.447%                                                                                     
          Out of the 1464096.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To        
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 14.6%                                                                                      
          The memory access pattern for shared loads might not be optimal and causes on average a 3.1 - way bank        
          conflict across all 480 shared load requests.This results in 536 bank conflicts,  which represent 35.83% of   
          the overall 1496 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.57
    Issued Warp Per Scheduler                        0.13
    No Eligible                                     87.43
    Active Warps Per Scheduler                       1.69
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.43%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.69 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.42
    Warp Cycles Per Executed Instruction                        13.42
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 61.44%                                                                                     
          On average, each warp of this workload spends 8.2 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.4% of the total average of 13.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.66
    Issued Instructions                                       300,534
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7388%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.39
    Achieved Active Warps Per SM                        6.65
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.43%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,461.88
    Total DRAM Elapsed Cycles                       5,508,352
    Average L1 Active Cycles                         4,102.86
    Total L1 Elapsed Cycles                         3,493,982
    Average L2 Active Cycles                        15,986.35
    Total L2 Elapsed Cycles                         4,597,240
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,102.86
    Total SM Elapsed Cycles                         3,493,982
    Average SMSP Active Cycles                       4,040.14
    Total SMSP Elapsed Cycles                      13,975,928
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.66%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.62% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.49%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.85% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.66%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.62% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                         3,977,500,000
    SM Frequency                        1,112,596,153.85
    Elapsed Cycles                                23,454
    Memory Throughput                              12.38
    DRAM Throughput                                12.38
    Duration                                      20,800
    L1/TEX Cache Throughput                        40.12
    L2 Cache Throughput                            12.11
    SM Active Cycles                            4,104.41
    Compute (SM) Throughput                         9.86
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.49
    Executed Ipc Elapsed                     0.08
    Issue Slots Busy                        12.37
    Issued Ipc Active                        0.49
    SM Busy                                 57.65
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (59.2%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        822,153,846,153.85
    Mem Busy                                              10.84
    Max Bandwidth                                         12.38
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           24.38
    Mem Pipes Busy                                         9.86
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.796%                                                                                     
          Out of the 1071104.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To        
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 14.39%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 3.1 - way bank        
          conflict across all 480 shared load requests.This results in 537 bank conflicts,  which represent 35.87% of   
          the overall 1497 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.41
    Issued Warp Per Scheduler                        0.12
    No Eligible                                     87.59
    Active Warps Per Scheduler                       1.65
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.59%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.65 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.32
    Warp Cycles Per Executed Instruction                        13.32
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 61.2%                                                                                      
          On average, each warp of this workload spends 8.2 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.2% of the total average of 13.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.60
    Executed Instructions                                     300,501
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7385%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.31
    Achieved Active Warps Per SM                        6.60
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.5%                                                                                      
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,350
    Total DRAM Elapsed Cycles                       5,294,848
    Average L1 Active Cycles                         4,104.41
    Total L1 Elapsed Cycles                         3,648,328
    Average L2 Active Cycles                        16,120.16
    Total L2 Elapsed Cycles                         4,413,792
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,104.41
    Total SM Elapsed Cycles                         3,648,328
    Average SMSP Active Cycles                       4,089.78
    Total SMSP Elapsed Cycles                      14,593,312
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.12%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.80% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.1%                                                                                      
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.94% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.12%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.80% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 5.598%                                                                                     
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.33% above the average, while the minimum instance value is 5.82% below the        
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,974,092,284.42
    SM Frequency                        1,120,685,987.14
    Elapsed Cycles                                23,824
    Memory Throughput                              12.04
    DRAM Throughput                                12.04
    Duration                                      21,152
    L1/TEX Cache Throughput                        40.12
    L2 Cache Throughput                            11.78
    SM Active Cycles                            4,104.52
    Compute (SM) Throughput                         9.90
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.49
    Executed Ipc Elapsed                     0.08
    Issue Slots Busy                        12.37
    Issued Ipc Active                        0.49
    SM Busy                                 57.65
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (59.3%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        798,971,255,673.22
    Mem Busy                                              10.63
    Max Bandwidth                                         12.04
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           22.59
    Mem Pipes Busy                                         9.90
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.646%                                                                                     
          Out of the 612096.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To         
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 14.06%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 3.1 - way bank        
          conflict across all 480 shared load requests.This results in 518 bank conflicts,  which represent 35.05% of   
          the overall 1478 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.47
    Issued Warp Per Scheduler                        0.12
    No Eligible                                     87.53
    Active Warps Per Scheduler                       1.66
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.53%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.66 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.32
    Warp Cycles Per Executed Instruction                        13.32
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 61.2%                                                                                      
          On average, each warp of this workload spends 8.2 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.2% of the total average of 13.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7385%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.31
    Achieved Active Warps Per SM                        6.60
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.5%                                                                                      
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,251.88
    Total DRAM Elapsed Cycles                       5,379,840
    Average L1 Active Cycles                         4,104.52
    Total L1 Elapsed Cycles                         3,637,722
    Average L2 Active Cycles                        15,246.91
    Total L2 Elapsed Cycles                         4,483,896
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,104.52
    Total SM Elapsed Cycles                         3,637,722
    Average SMSP Active Cycles                       4,069.93
    Total SMSP Elapsed Cycles                      14,550,888
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.15%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.74% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.04%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.75% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.15%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.74% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,985,339,506.17
    SM Frequency                        1,119,146,653.16
    Elapsed Cycles                                23,374
    Memory Throughput                              12.22
    DRAM Throughput                                12.22
    Duration                                      20,736
    L1/TEX Cache Throughput                        40.39
    L2 Cache Throughput                            11.96
    SM Active Cycles                            4,076.89
    Compute (SM) Throughput                        10.33
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.50
    Executed Ipc Elapsed                     0.09
    Issue Slots Busy                        12.45
    Issued Ipc Active                        0.50
    SM Busy                                 58.12
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (59.6%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        813,617,283,950.62
    Mem Busy                                              10.65
    Max Bandwidth                                         12.22
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           26.77
    Mem Pipes Busy                                        10.33
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.815%                                                                                     
          Out of the 548128.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To         
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 16.14%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 3.3 - way bank        
          conflict across all 480 shared load requests.This results in 639 bank conflicts,  which represent 39.96% of   
          the overall 1599 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.45
    Issued Warp Per Scheduler                        0.12
    No Eligible                                     87.55
    Active Warps Per Scheduler                       1.67
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.55%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.67 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.39
    Warp Cycles Per Executed Instruction                        13.39
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 61.66%                                                                                     
          On average, each warp of this workload spends 8.3 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.7% of the total average of 13.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7435%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.43
    Achieved Active Warps Per SM                        6.67
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.5%                                                                                      
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,237.88
    Total DRAM Elapsed Cycles                       5,288,960
    Average L1 Active Cycles                         4,076.89
    Total L1 Elapsed Cycles                         3,483,236
    Average L2 Active Cycles                        14,635.81
    Total L2 Elapsed Cycles                         4,408,732
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,076.89
    Total SM Elapsed Cycles                         3,483,236
    Average SMSP Active Cycles                       4,078.39
    Total SMSP Elapsed Cycles                      13,932,944
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.64%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.74% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.66%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.84% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.64%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.74% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (4, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,981,811,697.57
    SM Frequency                        1,080,532,275.32
    Elapsed Cycles                                24,809
    Memory Throughput                              16.24
    DRAM Throughput                                11.80
    Duration                                      22,432
    L1/TEX Cache Throughput                        39.54
    L2 Cache Throughput                            16.61
    SM Active Cycles                            8,329.82
    Compute (SM) Throughput                        19.31
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.4 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.49
    Executed Ipc Elapsed                     0.16
    Issue Slots Busy                        12.19
    Issued Ipc Active                        0.49
    SM Busy                                 56.81
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (58.7%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        784,057,061,340.94
    Mem Busy                                              15.60
    Max Bandwidth                                         16.24
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           50.43
    Mem Pipes Busy                                        19.31
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.312%                                                                                     
          Out of the 2054144.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To        
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 17.4%                                                                                      
          The memory access pattern for shared loads might not be optimal and causes on average a 3.6 - way bank        
          conflict across all 960 shared load requests.This results in 1509 bank conflicts,  which represent 44.01% of  
          the overall 3429 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.39
    Issued Warp Per Scheduler                        0.12
    No Eligible                                     87.61
    Active Warps Per Scheduler                       1.67
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.69%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.67 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.47
    Warp Cycles Per Executed Instruction                        13.47
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 60.05%                                                                                     
          On average, each warp of this workload spends 8.1 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.0% of the total average of 13.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                 1,015.19
    Executed Instructions                                     600,992
    Avg. Issued Instructions Per Scheduler                   1,015.30
    Issued Instructions                                       601,056
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7278%                                                                                    
          This kernel executes 0 fused and 1664 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 64
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                               16,384
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.43
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 56.76%                                                                                     
          The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.27
    Achieved Active Warps Per SM                        6.58
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.69%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,587.88
    Total DRAM Elapsed Cycles                       5,716,480
    Average L1 Active Cycles                         8,329.82
    Total L1 Elapsed Cycles                         3,745,182
    Average L2 Active Cycles                        17,490.83
    Total L2 Elapsed Cycles                         4,760,264
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         8,329.82
    Total SM Elapsed Cycles                         3,745,182
    Average SMSP Active Cycles                       8,193.51
    Total SMSP Elapsed Cycles                      14,980,728
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 19.24%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 58.45% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 18.96%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 58.56% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 19.24%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 58.45% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         39,936
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       4.05
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,983,542,899.41
    SM Frequency                        1,115,742,880.92
    Elapsed Cycles                                24,405
    Memory Throughput                              11.82
    DRAM Throughput                                11.82
    Duration                                      21,632
    L1/TEX Cache Throughput                        40.12
    L2 Cache Throughput                            11.56
    SM Active Cycles                            4,104.32
    Compute (SM) Throughput                        10.28
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.49
    Executed Ipc Elapsed                     0.09
    Issue Slots Busy                        12.37
    Issued Ipc Active                        0.49
    SM Busy                                 57.72
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (59.3%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        785,218,934,911.24
    Mem Busy                                              10.36
    Max Bandwidth                                         11.82
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           25.43
    Mem Pipes Busy                                        10.28
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.409%                                                                                     
          Out of the 808576.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To         
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 14.41%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 3.1 - way bank        
          conflict across all 480 shared load requests.This results in 538 bank conflicts,  which represent 35.91% of   
          the overall 1498 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.55
    Issued Warp Per Scheduler                        0.13
    No Eligible                                     87.45
    Active Warps Per Scheduler                       1.67
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.45%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.67 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.32
    Warp Cycles Per Executed Instruction                        13.32
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 61.43%                                                                                     
          On average, each warp of this workload spends 8.2 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.4% of the total average of 13.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7385%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.30
    Achieved Active Warps Per SM                        6.60
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.45%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,293.88
    Total DRAM Elapsed Cycles                       5,515,008
    Average L1 Active Cycles                         4,104.32
    Total L1 Elapsed Cycles                         3,504,126
    Average L2 Active Cycles                        15,170.67
    Total L2 Elapsed Cycles                         4,590,616
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,104.32
    Total SM Elapsed Cycles                         3,504,126
    Average SMSP Active Cycles                       4,046.58
    Total SMSP Elapsed Cycles                      14,016,504
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.65%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.74% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.47%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.79% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.65%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.74% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,981,873,111.78
    SM Frequency                        1,109,363,198.64
    Elapsed Cycles                                23,879
    Memory Throughput                              12.28
    DRAM Throughput                                12.28
    Duration                                      21,184
    L1/TEX Cache Throughput                        40.60
    L2 Cache Throughput                            12.01
    SM Active Cycles                            4,055.91
    Compute (SM) Throughput                        10.30
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.50
    Executed Ipc Elapsed                     0.09
    Issue Slots Busy                        12.52
    Issued Ipc Active                        0.50
    SM Busy                                 58.36
    -------------------- ----------- ------------

    OPT   TMEM (Tensor Memory) is the highest-utilized pipeline (60.2%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. The pipeline is well-utilized, but might become a bottleneck if more work is added. Based on the  
          number of executed instructions, the highest utilized pipeline (7.9%) is ALU. It executes integer and logic   
          operations. Comparing the two, the overall pipeline utilization appears to be caused by high-latency          
          instructions. See the Kernel Profiling Guide                                                                  
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        815,371,601,208.46
    Mem Busy                                              11.12
    Max Bandwidth                                         12.28
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           21.69
    Mem Pipes Busy                                        10.30
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.649%                                                                                     
          Out of the 1331776.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To        
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 16.67%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 3.4 - way bank        
          conflict across all 480 shared load requests.This results in 669 bank conflicts,  which represent 41.07% of   
          the overall 1629 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.41
    Issued Warp Per Scheduler                        0.12
    No Eligible                                     87.59
    Active Warps Per Scheduler                       1.67
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.59%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.67 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.44
    Warp Cycles Per Executed Instruction                        13.45
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 61.23%                                                                                     
          On average, each warp of this workload spends 8.2 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.2% of the total average of 13.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7473%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.53
    Achieved Active Warps Per SM                        6.74
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.5%                                                                                      
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,434
    Total DRAM Elapsed Cycles                       5,398,528
    Average L1 Active Cycles                         4,055.91
    Total L1 Elapsed Cycles                         3,505,376
    Average L2 Active Cycles                        15,794.41
    Total L2 Elapsed Cycles                         4,493,280
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,055.91
    Total SM Elapsed Cycles                         3,505,376
    Average SMSP Active Cycles                       4,089.63
    Total SMSP Elapsed Cycles                      14,021,504
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.46%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.63% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.62%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.89% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.46%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.63% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                         3,976,562,500
    SM Frequency                        1,112,060,546.88
    Elapsed Cycles                                24,264
    Memory Throughput                              11.85
    DRAM Throughput                                11.85
    Duration                                      21,504
    L1/TEX Cache Throughput                        40.10
    L2 Cache Throughput                            11.60
    SM Active Cycles                            4,106.05
    Compute (SM) Throughput                         9.89
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.49
    Executed Ipc Elapsed                     0.08
    Issue Slots Busy                        12.36
    Issued Ipc Active                        0.49
    SM Busy                                 57.61
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (59.3%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        787,226,190,476.19
    Mem Busy                                              10.47
    Max Bandwidth                                         11.85
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           24.70
    Mem Pipes Busy                                         9.89
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.482%                                                                                     
          Out of the 677920.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To         
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 14.54%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 3.1 - way bank        
          conflict across all 480 shared load requests.This results in 546 bank conflicts,  which represent 36.25% of   
          the overall 1506 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.55
    Issued Warp Per Scheduler                        0.13
    No Eligible                                     87.45
    Active Warps Per Scheduler                       1.67
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.45%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.67 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.29
    Warp Cycles Per Executed Instruction                        13.29
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 61.38%                                                                                     
          On average, each warp of this workload spends 8.2 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.4% of the total average of 13.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7382%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.28
    Achieved Active Warps Per SM                        6.58
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.45%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,265.88
    Total DRAM Elapsed Cycles                       5,472,768
    Average L1 Active Cycles                         4,106.05
    Total L1 Elapsed Cycles                         3,645,866
    Average L2 Active Cycles                        15,281.40
    Total L2 Elapsed Cycles                         4,562,188
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,106.05
    Total SM Elapsed Cycles                         3,645,866
    Average SMSP Active Cycles                       4,045.54
    Total SMSP Elapsed Cycles                      14,583,464
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.11%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.67% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 12.94%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.78% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.11%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.67% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,976,954,277.29
    SM Frequency                        1,110,884,494.84
    Elapsed Cycles                                24,465
    Memory Throughput                              12.03
    DRAM Throughput                                12.03
    Duration                                      21,696
    L1/TEX Cache Throughput                        40.95
    L2 Cache Throughput                            11.77
    SM Active Cycles                            4,079.82
    Compute (SM) Throughput                        10.22
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.50
    Executed Ipc Elapsed                     0.09
    Issue Slots Busy                        12.44
    Issued Ipc Active                        0.50
    SM Busy                                 58.06
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (59.6%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        798,772,861,356.93
    Mem Busy                                              10.86
    Max Bandwidth                                         12.03
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           23.99
    Mem Pipes Busy                                        10.22
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.425%                                                                                     
          Out of the 1462816.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To        
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 24.13%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 4.9 - way bank        
          conflict across all 480 shared load requests.This results in 1377 bank conflicts,  which represent 58.92% of  
          the overall 2337 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.06
    Issued Warp Per Scheduler                        0.12
    No Eligible                                     87.94
    Active Warps Per Scheduler                       1.63
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.94%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.63 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.47
    Warp Cycles Per Executed Instruction                        13.47
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 61.42%                                                                                     
          On average, each warp of this workload spends 8.3 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.4% of the total average of 13.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7429%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.49
    Achieved Active Warps Per SM                        6.71
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.5%                                                                                      
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,462
    Total DRAM Elapsed Cycles                       5,522,176
    Average L1 Active Cycles                         4,079.82
    Total L1 Elapsed Cycles                         3,518,692
    Average L2 Active Cycles                        16,034.75
    Total L2 Elapsed Cycles                         4,600,828
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,079.82
    Total SM Elapsed Cycles                         3,518,692
    Average SMSP Active Cycles                       4,207.70
    Total SMSP Elapsed Cycles                      14,074,768
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.48%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.58% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.98%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.02% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.48%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.58% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,975,843,558.28
    SM Frequency                        1,108,488,305.21
    Elapsed Cycles                                23,449
    Memory Throughput                              12.34
    DRAM Throughput                                12.34
    Duration                                      20,864
    L1/TEX Cache Throughput                        39.66
    L2 Cache Throughput                            12.07
    SM Active Cycles                            4,152.49
    Compute (SM) Throughput                        10.08
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.49
    Executed Ipc Elapsed                     0.08
    Issue Slots Busy                        12.23
    Issued Ipc Active                        0.49
    SM Busy                                 57.03
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (58.7%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        819,619,631,901.84
    Mem Busy                                              10.81
    Max Bandwidth                                         12.34
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           24.21
    Mem Pipes Busy                                        10.08
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.772%                                                                                     
          Out of the 1070752.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To        
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 14.66%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 3.2 - way bank        
          conflict across all 480 shared load requests.This results in 563 bank conflicts,  which represent 36.97% of   
          the overall 1523 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.37
    Issued Warp Per Scheduler                        0.12
    No Eligible                                     87.63
    Active Warps Per Scheduler                       1.68
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.63%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.1 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.68 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.60
    Warp Cycles Per Executed Instruction                        13.60
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 59.71%                                                                                     
          On average, each warp of this workload spends 8.1 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 59.7% of the total average of 13.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7299%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.40
    Achieved Active Warps Per SM                        6.66
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.5%                                                                                      
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,349.88
    Total DRAM Elapsed Cycles                       5,308,928
    Average L1 Active Cycles                         4,152.49
    Total L1 Elapsed Cycles                         3,577,840
    Average L2 Active Cycles                        15,322.05
    Total L2 Elapsed Cycles                         4,425,844
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,152.49
    Total SM Elapsed Cycles                         3,577,840
    Average SMSP Active Cycles                       4,102.54
    Total SMSP Elapsed Cycles                      14,311,360
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.52%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.72% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.36%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.75% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.52%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.72% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                         3,976,562,500
    SM Frequency                        1,116,135,370.16
    Elapsed Cycles                                24,186
    Memory Throughput                              11.83
    DRAM Throughput                                11.83
    Duration                                      21,504
    L1/TEX Cache Throughput                        40.11
    L2 Cache Throughput                            11.57
    SM Active Cycles                            4,105.71
    Compute (SM) Throughput                        10.04
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.49
    Executed Ipc Elapsed                     0.08
    Issue Slots Busy                        12.36
    Issued Ipc Active                        0.49
    SM Busy                                 57.73
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (59.3%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        785,892,857,142.86
    Mem Busy                                              10.51
    Max Bandwidth                                         11.83
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           25.59
    Mem Pipes Busy                                        10.04
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.486%                                                                                     
          Out of the 610464.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To         
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 14.73%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 3.2 - way bank        
          conflict across all 480 shared load requests.This results in 557 bank conflicts,  which represent 36.72% of   
          the overall 1517 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.48
    Issued Warp Per Scheduler                        0.12
    No Eligible                                     87.52
    Active Warps Per Scheduler                       1.66
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.52%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.66 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.31
    Warp Cycles Per Executed Instruction                        13.31
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 60.87%                                                                                     
          On average, each warp of this workload spends 8.1 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.9% of the total average of 13.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.60
    Executed Instructions                                     300,501
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7383%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.29
    Achieved Active Warps Per SM                        6.59
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.5%                                                                                      
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,251.88
    Total DRAM Elapsed Cycles                       5,472,768
    Average L1 Active Cycles                         4,105.71
    Total L1 Elapsed Cycles                         3,585,560
    Average L2 Active Cycles                        15,181.01
    Total L2 Elapsed Cycles                         4,564,028
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,105.71
    Total SM Elapsed Cycles                         3,585,560
    Average SMSP Active Cycles                       4,066.31
    Total SMSP Elapsed Cycles                      14,342,240
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.33%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.66% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.24%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.89% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.33%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.66% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,975,155,279.50
    SM Frequency                        1,119,352,921.20
    Elapsed Cycles                                23,194
    Memory Throughput                              12.33
    DRAM Throughput                                12.33
    Duration                                      20,608
    L1/TEX Cache Throughput                        39.76
    L2 Cache Throughput                            12.06
    SM Active Cycles                            4,141.46
    Compute (SM) Throughput                        10.36
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.49
    Executed Ipc Elapsed                     0.09
    Issue Slots Busy                        12.26
    Issued Ipc Active                        0.49
    SM Busy                                 57.18
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (58.7%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        818,683,229,813.66
    Mem Busy                                              10.79
    Max Bandwidth                                         12.33
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           25.22
    Mem Pipes Busy                                        10.36
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.908%                                                                                     
          Out of the 546080.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To         
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 20.72%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 4.2 - way bank        
          conflict across all 480 shared load requests.This results in 1045 bank conflicts,  which represent 52.12% of  
          the overall 2005 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.42
    Issued Warp Per Scheduler                        0.12
    No Eligible                                     87.58
    Active Warps Per Scheduler                       1.67
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.58%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.67 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.46
    Warp Cycles Per Executed Instruction                        13.47
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 61.17%                                                                                     
          On average, each warp of this workload spends 8.2 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.2% of the total average of 13.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7319%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.33
    Achieved Active Warps Per SM                        6.61
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.5%                                                                                      
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,238
    Total DRAM Elapsed Cycles                       5,242,880
    Average L1 Active Cycles                         4,141.46
    Total L1 Elapsed Cycles                         3,469,584
    Average L2 Active Cycles                        14,831.21
    Total L2 Elapsed Cycles                         4,371,380
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,141.46
    Total SM Elapsed Cycles                         3,469,584
    Average SMSP Active Cycles                       4,085.85
    Total SMSP Elapsed Cycles                      13,878,336
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.91%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.73% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.74%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.86% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.91%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.73% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (4, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,984,804,630.97
    SM Frequency                        1,090,052,912.45
    Elapsed Cycles                                24,661
    Memory Throughput                              16.36
    DRAM Throughput                                11.96
    Duration                                      22,112
    L1/TEX Cache Throughput                        39.34
    L2 Cache Throughput                            17.11
    SM Active Cycles                            8,370.96
    Compute (SM) Throughput                        19.38
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.4 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.49
    Executed Ipc Elapsed                     0.16
    Issue Slots Busy                        12.13
    Issued Ipc Active                        0.49
    SM Busy                                 56.52
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (58.1%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        795,403,762,662.81
    Mem Busy                                              16.04
    Max Bandwidth                                         16.36
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           46.43
    Mem Pipes Busy                                        19.38
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.446%                                                                                     
          Out of the 2054656.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To        
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 17.96%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 3.7 - way bank        
          conflict across all 960 shared load requests.This results in 1613 bank conflicts,  which represent 45.66% of  
          the overall 3533 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.26
    Issued Warp Per Scheduler                        0.12
    No Eligible                                     87.74
    Active Warps Per Scheduler                       1.65
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.62%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.65 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.43
    Warp Cycles Per Executed Instruction                        13.43
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 60.64%                                                                                     
          On average, each warp of this workload spends 8.1 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.6% of the total average of 13.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                 1,015.19
    Executed Instructions                                     600,992
    Avg. Issued Instructions Per Scheduler                   1,015.30
    Issued Instructions                                       601,056
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7242%                                                                                    
          This kernel executes 0 fused and 1664 non-fused FP32 instructions. By converting pairs of non-fused           
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 64
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                               16,384
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.43
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 56.76%                                                                                     
          The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.19
    Achieved Active Warps Per SM                        6.52
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.62%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,587.88
    Total DRAM Elapsed Cycles                       5,639,168
    Average L1 Active Cycles                         8,370.96
    Total L1 Elapsed Cycles                         3,713,908
    Average L2 Active Cycles                        17,750.04
    Total L2 Elapsed Cycles                         4,697,244
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         8,370.96
    Total SM Elapsed Cycles                         3,713,908
    Average SMSP Active Cycles                       8,281.40
    Total SMSP Elapsed Cycles                      14,855,632
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 19.41%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 58.20% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 19.38%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 58.73% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 19.41%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 58.20% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         39,936
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       4.05
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,977,136,431.78
    SM Frequency                        1,116,496,439.28
    Elapsed Cycles                                24,082
    Memory Throughput                              11.97
    DRAM Throughput                                11.97
    Duration                                      21,344
    L1/TEX Cache Throughput                        40.20
    L2 Cache Throughput                            11.71
    SM Active Cycles                            4,095.81
    Compute (SM) Throughput                        10.42
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.50
    Executed Ipc Elapsed                     0.09
    Issue Slots Busy                        12.39
    Issued Ipc Active                        0.50
    SM Busy                                 57.89
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (59.4%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        795,826,086,956.52
    Mem Busy                                              10.76
    Max Bandwidth                                         11.97
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           24.92
    Mem Pipes Busy                                        10.42
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.549%                                                                                     
          Out of the 806816.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To         
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 21.29%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 4.3 - way bank        
          conflict across all 480 shared load requests.This results in 1081 bank conflicts,  which represent 52.96% of  
          the overall 2041 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.57
    Issued Warp Per Scheduler                        0.13
    No Eligible                                     87.43
    Active Warps Per Scheduler                       1.68
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.43%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.68 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.35
    Warp Cycles Per Executed Instruction                        13.35
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 61.62%                                                                                     
          On average, each warp of this workload spends 8.2 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.6% of the total average of 13.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.74%                                                                                      
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.35
    Achieved Active Warps Per SM                        6.62
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.43%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,294
    Total DRAM Elapsed Cycles                       5,432,832
    Average L1 Active Cycles                         4,095.81
    Total L1 Elapsed Cycles                         3,458,490
    Average L2 Active Cycles                        15,021.76
    Total L2 Elapsed Cycles                         4,531,552
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,095.81
    Total SM Elapsed Cycles                         3,458,490
    Average SMSP Active Cycles                       4,037.51
    Total SMSP Elapsed Cycles                      13,833,960
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.81%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.81% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.64%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.95% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.81%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.81% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

  cutlass3x_sm100_bstensorop_s256x192x64gemm_block_scaled_ue4m3xf4_ue4m3xf4_f32_f16_f16_256x192x256_0_tnn_align32_o_vs16_2sm_bias_f16_relu (2, 16, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,981,510,015.41
    SM Frequency                        1,111,559,851.69
    Elapsed Cycles                                23,334
    Memory Throughput                              12.52
    DRAM Throughput                                12.52
    Duration                                      20,768
    L1/TEX Cache Throughput                        40.42
    L2 Cache Throughput                            12.25
    SM Active Cycles                            4,074.45
    Compute (SM) Throughput                        10.23
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved      
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.50
    Executed Ipc Elapsed                     0.09
    Issue Slots Busy                        12.46
    Issued Ipc Active                        0.50
    SM Busy                                 58.03
    -------------------- ----------- ------------

    INF   TMEM (Tensor Memory) is the highest-utilized pipeline (59.7%) based on active cycles, taking into account the 
          rates of its different instructions. It increments for LDT(M), STT(M), UTCCP, UTCMMA and UTCSHIFT             
          operations. It is well-utilized, but should not be a bottleneck.                                              

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        831,691,833,590.14
    Mem Busy                                              11.19
    Max Bandwidth                                         12.52
    L1/TEX Hit Rate                                           0
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           25.07
    Mem Pipes Busy                                        10.23
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 9.831%                                                                                     
          Out of the 1332704.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To        
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 21.86%                                                                                     
          The memory access pattern for shared loads might not be optimal and causes on average a 4.4 - way bank        
          conflict across all 480 shared load requests.This results in 1131 bank conflicts,  which represent 54.09% of  
          the overall 2091 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads. 

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                            12.45
    Issued Warp Per Scheduler                        0.12
    No Eligible                                     87.55
    Active Warps Per Scheduler                       1.66
    Eligible Warps Per Scheduler                     0.13
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.48%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.66 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          13.32
    Warp Cycles Per Executed Instruction                        13.32
    Avg. Active Threads Per Warp                                28.27
    Avg. Not Predicated Off Threads Per Warp                    27.02
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 60.93%                                                                                     
          On average, each warp of this workload spends 8.1 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.9% of the total average of 13.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   507.59
    Executed Instructions                                     300,496
    Avg. Issued Instructions Per Scheduler                     507.65
    Issued Instructions                                       300,528
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 0.7439%                                                                                    
          This kernel executes 0 fused and 832 non-fused FP32 instructions. By converting pairs of non-fused            
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               256
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     158
    Shared Memory Configuration Size                     233,472
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                      227,328
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                8,192
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.22
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                              12.50
    Achieved Occupancy                                 10.39
    Achieved Active Warps Per SM                        6.65
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.48%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,433.88
    Total DRAM Elapsed Cycles                       5,292,032
    Average L1 Active Cycles                         4,074.45
    Total L1 Elapsed Cycles                         3,515,912
    Average L2 Active Cycles                        15,685.61
    Total L2 Elapsed Cycles                         4,407,628
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                         4,074.45
    Total SM Elapsed Cycles                         3,515,912
    Average SMSP Active Cycles                       4,078.33
    Total SMSP Elapsed Cycles                      14,063,648
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 13.48%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.62% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.56%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.99% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 13.48%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.62% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 5.261%                                                                                     
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.03% above the average, while the minimum instance value is 5.56% below the        
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.07
    Branch Instructions                         19,968
    Branch Efficiency                            93.82
    Avg. Divergent Branches                       2.03
    ------------------------- ----------- ------------

