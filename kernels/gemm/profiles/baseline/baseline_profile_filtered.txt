[28048] python3.10@127.0.0.1
  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,994,783,616.69
    SM Frequency                        1,121,255,899.48
    Elapsed Cycles                               163,401
    Memory Throughput                              11.68
    DRAM Throughput                                 7.04
    Duration                                     144,928
    L1/TEX Cache Throughput                        32.81
    L2 Cache Throughput                             8.83
    SM Active Cycles                           57,862.72
    Compute (SM) Throughput                         5.65
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 15.87
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 84.13%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        468,364,760,432.77
    Mem Busy                                               8.20
    Max Bandwidth                                         11.68
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           39.72
    Mem Pipes Busy                                         4.92
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.79%                                                                                      
          Out of the 29414848.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 10.95%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.94
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.06
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.32%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 106.7 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         106.49
    Warp Cycles Per Executed Instruction                       107.20
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    29.59
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.32%                                                                                     
          On average, each warp of this workload spends 94.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.4% of the total average of 106.5 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.73
    Executed Instructions                                     318,336
    Avg. Issued Instructions Per Scheduler                     541.29
    Issued Instructions                                       320,443
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.13
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.23
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.79%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,144.12
    Total DRAM Elapsed Cycles                      37,053,184
    Average L1 Active Cycles                        57,862.72
    Total L1 Elapsed Cycles                        24,054,940
    Average L2 Active Cycles                       158,573.71
    Total L2 Elapsed Cycles                        30,792,676
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,862.72
    Total SM Elapsed Cycles                        24,054,940
    Average SMSP Active Cycles                      57,734.22
    Total SMSP Elapsed Cycles                      96,219,760
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.53%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.27% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.47%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.26% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.53%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.27% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,620
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.83%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,994,618,834.08
    SM Frequency                        1,121,474,740.75
    Elapsed Cycles                               160,851
    Memory Throughput                              11.85
    DRAM Throughput                                 7.18
    Duration                                     142,720
    L1/TEX Cache Throughput                        33.06
    L2 Cache Throughput                             8.97
    SM Active Cycles                           57,426.46
    Compute (SM) Throughput                         5.73
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 15.99
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 84.01%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        477,530,044,843.05
    Mem Busy                                               8.29
    Max Bandwidth                                         11.85
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           39.26
    Mem Pipes Busy                                         4.99
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.874%                                                                                     
          Out of the 29443968.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.11%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.94
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.06
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.15%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 105.9 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         105.91
    Warp Cycles Per Executed Instruction                       106.62
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    29.60
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.15%                                                                                     
          On average, each warp of this workload spends 93.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.2% of the total average of 105.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.63
    Executed Instructions                                     318,276
    Avg. Issued Instructions Per Scheduler                     541.19
    Issued Instructions                                       320,385
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.72%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,277.88
    Total DRAM Elapsed Cycles                      36,487,168
    Average L1 Active Cycles                        57,426.46
    Total L1 Elapsed Cycles                        23,713,180
    Average L2 Active Cycles                       155,896.25
    Total L2 Elapsed Cycles                        30,321,176
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,426.46
    Total SM Elapsed Cycles                        23,713,180
    Average SMSP Active Cycles                      57,307.34
    Total SMSP Elapsed Cycles                      94,852,720
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.67%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.25% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.63%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.27% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.67%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.25% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,618
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.69%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,745,811.93
    SM Frequency                        1,121,495,106.94
    Elapsed Cycles                               161,466
    Memory Throughput                              11.90
    DRAM Throughput                                 7.12
    Duration                                     143,264
    L1/TEX Cache Throughput                        33.02
    L2 Cache Throughput                             8.99
    SM Active Cycles                           57,501.59
    Compute (SM) Throughput                         5.75
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 15.97
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 84.03%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        473,756,533,392.90
    Mem Busy                                               8.45
    Max Bandwidth                                         11.90
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           37.39
    Mem Pipes Busy                                         5.01
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.863%                                                                                     
          Out of the 29445920.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.16%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.94
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.06
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.1%                                                                                      
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 106.3 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         106.09
    Warp Cycles Per Executed Instruction                       106.77
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    29.60
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.1%                                                                                      
          On average, each warp of this workload spends 93.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.2% of the total average of 106.1 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.77
    Executed Instructions                                     318,360
    Avg. Issued Instructions Per Scheduler                     541.19
    Issued Instructions                                       320,385
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.71%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,140.75
    Total DRAM Elapsed Cycles                      36,618,240
    Average L1 Active Cycles                        57,501.59
    Total L1 Elapsed Cycles                        23,611,438
    Average L2 Active Cycles                       156,880.79
    Total L2 Elapsed Cycles                        30,433,692
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,501.59
    Total SM Elapsed Cycles                        23,611,438
    Average SMSP Active Cycles                      57,522.86
    Total SMSP Elapsed Cycles                      94,445,752
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.8%                                                                                      
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.26% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.74%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.06% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.8%                                                                                      
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.26% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,634
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.92%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,994,095,816.46
    SM Frequency                        1,120,649,179.04
    Elapsed Cycles                               160,302
    Memory Throughput                              11.91
    DRAM Throughput                                 7.21
    Duration                                     142,272
    L1/TEX Cache Throughput                        33.13
    L2 Cache Throughput                             9.00
    SM Active Cycles                           57,305.65
    Compute (SM) Throughput                         5.76
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 16.02
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 83.98%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        479,735,492,577.60
    Mem Busy                                               8.43
    Max Bandwidth                                         11.91
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           39.80
    Mem Pipes Busy                                         5.01
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.878%                                                                                     
          Out of the 29415008.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.17%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.95
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.05
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.09%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 105.8 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         105.60
    Warp Cycles Per Executed Instruction                       106.30
    Avg. Active Threads Per Warp                                31.86
    Avg. Not Predicated Off Threads Per Warp                    29.60
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.09%                                                                                     
          On average, each warp of this workload spends 93.2 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.2% of the total average of 105.6 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.67
    Executed Instructions                                     318,300
    Avg. Issued Instructions Per Scheduler                     541.20
    Issued Instructions                                       320,390
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.23
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.75%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,326.62
    Total DRAM Elapsed Cycles                      36,367,872
    Average L1 Active Cycles                        57,305.65
    Total L1 Elapsed Cycles                        23,585,026
    Average L2 Active Cycles                       156,587.09
    Total L2 Elapsed Cycles                        30,222,000
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,305.65
    Total SM Elapsed Cycles                        23,585,026
    Average SMSP Active Cycles                      57,246.33
    Total SMSP Elapsed Cycles                      94,340,104
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.67%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.04% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.68%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.13% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.67%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.04% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,622
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 89.38%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,983,657.94
    SM Frequency                        1,121,338,601.13
    Elapsed Cycles                               161,121
    Memory Throughput                              11.89
    DRAM Throughput                                 7.14
    Duration                                     142,944
    L1/TEX Cache Throughput                        32.98
    L2 Cache Throughput                             8.98
    SM Active Cycles                           57,574.79
    Compute (SM) Throughput                         5.75
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 15.95
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 84.05%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        474,706,066,711.44
    Mem Busy                                               8.33
    Max Bandwidth                                         11.89
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           37.78
    Mem Pipes Busy                                         5.00
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.87%                                                                                      
          Out of the 29450048.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.14%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.94
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.06
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.11%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 106.3 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         106.17
    Warp Cycles Per Executed Instruction                       106.88
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    29.60
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.11%                                                                                     
          On average, each warp of this workload spends 93.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.1% of the total average of 106.2 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.68
    Executed Instructions                                     318,304
    Avg. Issued Instructions Per Scheduler                     541.27
    Issued Instructions                                       320,433
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.72%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                         33,133
    Total DRAM Elapsed Cycles                      36,538,624
    Average L1 Active Cycles                        57,574.79
    Total L1 Elapsed Cycles                        23,642,262
    Average L2 Active Cycles                       156,707.77
    Total L2 Elapsed Cycles                        30,368,648
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,574.79
    Total SM Elapsed Cycles                        23,642,262
    Average SMSP Active Cycles                      57,557.03
    Total SMSP Elapsed Cycles                      94,569,048
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.68%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 62.93% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.72%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.05% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.68%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 62.93% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,627
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 89.01%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,879,155.44
    SM Frequency                        1,120,426,914.87
    Elapsed Cycles                               160,593
    Memory Throughput                              11.91
    DRAM Throughput                                 7.19
    Duration                                     142,464
    L1/TEX Cache Throughput                        33.11
    L2 Cache Throughput                             8.97
    SM Active Cycles                           57,350.01
    Compute (SM) Throughput                         5.76
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 16.01
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 83.99%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        478,289,308,176.10
    Mem Busy                                               8.40
    Max Bandwidth                                         11.91
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           37.01
    Mem Pipes Busy                                         5.01
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.889%                                                                                     
          Out of the 29440736.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.17%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.95
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.05
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.09%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 105.6 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         105.84
    Warp Cycles Per Executed Instruction                       106.53
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    29.60
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.09%                                                                                     
          On average, each warp of this workload spends 93.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.3% of the total average of 105.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.67
    Executed Instructions                                     318,300
    Avg. Issued Instructions Per Scheduler                     541.20
    Issued Instructions                                       320,388
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        4.00
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.7%                                                                                      
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                         33,271
    Total DRAM Elapsed Cycles                      36,414,976
    Average L1 Active Cycles                        57,350.01
    Total L1 Elapsed Cycles                        23,587,176
    Average L2 Active Cycles                       155,450.49
    Total L2 Elapsed Cycles                        30,263,860
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,350.01
    Total SM Elapsed Cycles                        23,587,176
    Average SMSP Active Cycles                      57,128.25
    Total SMSP Elapsed Cycles                      94,348,704
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.7%                                                                                      
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.08% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.59%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.01% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.7%                                                                                      
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.08% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,608
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.6%                                                                                      
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,731,811.06
    SM Frequency                        1,120,385,430.66
    Elapsed Cycles                               161,024
    Memory Throughput                              11.79
    DRAM Throughput                                 7.13
    Duration                                     142,944
    L1/TEX Cache Throughput                        33.00
    L2 Cache Throughput                             8.97
    SM Active Cycles                           57,544.14
    Compute (SM) Throughput                         5.70
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 15.96
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 84.04%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        474,353,257,219.61
    Mem Busy                                               8.41
    Max Bandwidth                                         11.79
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           37.59
    Mem Pipes Busy                                         4.96
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.881%                                                                                     
          Out of the 29443648.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.05%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.94
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.06
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.21%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 106.1 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         106.64
    Warp Cycles Per Executed Instruction                       107.33
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    29.60
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.94%                                                                                     
          On average, each warp of this workload spends 93.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 87.9% of the total average of 106.6 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.73
    Executed Instructions                                     318,336
    Avg. Issued Instructions Per Scheduler                     541.21
    Issued Instructions                                       320,399
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.27
    Achieved Active Warps Per SM                        4.01
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.57%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,108.38
    Total DRAM Elapsed Cycles                      36,536,320
    Average L1 Active Cycles                        57,544.14
    Total L1 Elapsed Cycles                        23,835,884
    Average L2 Active Cycles                       157,283.39
    Total L2 Elapsed Cycles                        30,363,956
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,544.14
    Total SM Elapsed Cycles                        23,835,884
    Average SMSP Active Cycles                      57,444.35
    Total SMSP Elapsed Cycles                      95,343,536
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.57%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.16% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.56%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.26% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.57%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.16% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,635
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 89.35%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,775,235.53
    SM Frequency                        1,120,966,696.11
    Elapsed Cycles                               160,759
    Memory Throughput                              11.82
    DRAM Throughput                                 7.19
    Duration                                     142,656
    L1/TEX Cache Throughput                        33.14
    L2 Cache Throughput                             9.00
    SM Active Cycles                           57,298.79
    Compute (SM) Throughput                         5.72
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 16.02
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 83.98%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        477,943,472,409.15
    Mem Busy                                               8.42
    Max Bandwidth                                         11.82
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           37.43
    Mem Pipes Busy                                         4.97
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.87%                                                                                      
          Out of the 29442688.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.08%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.95
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.05
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.18%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 105.7 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         105.62
    Warp Cycles Per Executed Instruction                       106.33
    Avg. Active Threads Per Warp                                31.86
    Avg. Not Predicated Off Threads Per Warp                    29.61
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.18%                                                                                     
          On average, each warp of this workload spends 93.2 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.3% of the total average of 105.6 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.63
    Executed Instructions                                     318,276
    Avg. Issued Instructions Per Scheduler                     541.25
    Issued Instructions                                       320,419
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.74%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,291.75
    Total DRAM Elapsed Cycles                      36,463,104
    Average L1 Active Cycles                        57,298.79
    Total L1 Elapsed Cycles                        23,771,918
    Average L2 Active Cycles                       156,546.07
    Total L2 Elapsed Cycles                        30,303,236
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,298.79
    Total SM Elapsed Cycles                        23,771,918
    Average SMSP Active Cycles                      57,195.16
    Total SMSP Elapsed Cycles                      95,087,672
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.59%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.33% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.47%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.11% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.59%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.33% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,621
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 89.11%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,226,600.99
    SM Frequency                        1,120,411,511.98
    Elapsed Cycles                               161,017
    Memory Throughput                              11.74
    DRAM Throughput                                 7.14
    Duration                                     142,912
    L1/TEX Cache Throughput                        32.99
    L2 Cache Throughput                             8.99
    SM Active Cycles                           57,553.07
    Compute (SM) Throughput                         5.68
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 15.95
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 84.05%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        474,830,273,175.10
    Mem Busy                                               8.33
    Max Bandwidth                                         11.74
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           39.02
    Mem Pipes Busy                                         4.94
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.88%                                                                                      
          Out of the 29441728.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.01%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.94
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.06
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.26%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 106.3 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         106.38
    Warp Cycles Per Executed Instruction                       107.07
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    29.60
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.97%                                                                                     
          On average, each warp of this workload spends 93.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.0% of the total average of 106.4 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.68
    Executed Instructions                                     318,308
    Avg. Issued Instructions Per Scheduler                     541.17
    Issued Instructions                                       320,371
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.25
    Achieved Active Warps Per SM                        4.00
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.65%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,134.25
    Total DRAM Elapsed Cycles                      36,523,520
    Average L1 Active Cycles                        57,553.07
    Total L1 Elapsed Cycles                        23,927,940
    Average L2 Active Cycles                       156,687.07
    Total L2 Elapsed Cycles                        30,356,504
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,553.07
    Total SM Elapsed Cycles                        23,927,940
    Average SMSP Active Cycles                      57,499.95
    Total SMSP Elapsed Cycles                      95,711,760
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.39%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 62.91% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.41%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.01% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.39%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 62.91% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,640
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 89.04%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,994,545,859.31
    SM Frequency                        1,121,105,924.42
    Elapsed Cycles                               162,044
    Memory Throughput                              11.84
    DRAM Throughput                                 7.13
    Duration                                     143,744
    L1/TEX Cache Throughput                        33.11
    L2 Cache Throughput                             8.91
    SM Active Cycles                           57,341.18
    Compute (SM) Throughput                         5.73
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 16.01
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 83.99%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        474,254,674,977.74
    Mem Busy                                               8.29
    Max Bandwidth                                         11.84
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           37.76
    Mem Pipes Busy                                         4.98
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.833%                                                                                     
          Out of the 29444192.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.1%                                                                                      
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.95
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.05
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.16%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 105.6 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         105.86
    Warp Cycles Per Executed Instruction                       106.55
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    29.60
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.16%                                                                                     
          On average, each warp of this workload spends 93.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.3% of the total average of 105.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.68
    Executed Instructions                                     318,308
    Avg. Issued Instructions Per Scheduler                     541.18
    Issued Instructions                                       320,380
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        4.00
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.69%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,286.75
    Total DRAM Elapsed Cycles                      36,748,288
    Average L1 Active Cycles                        57,341.18
    Total L1 Elapsed Cycles                        23,725,456
    Average L2 Active Cycles                       157,226.91
    Total L2 Elapsed Cycles                        30,537,652
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,341.18
    Total SM Elapsed Cycles                        23,725,456
    Average SMSP Active Cycles                      57,144.45
    Total SMSP Elapsed Cycles                      94,901,824
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.59%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.16% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.55%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.25% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.59%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.16% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,598
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.81%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,992,869,875.22
    SM Frequency                        1,121,568,975.60
    Elapsed Cycles                               161,855
    Memory Throughput                              11.93
    DRAM Throughput                                 7.10
    Duration                                     143,616
    L1/TEX Cache Throughput                        32.99
    L2 Cache Throughput                             8.86
    SM Active Cycles                           57,559.07
    Compute (SM) Throughput                         5.77
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 15.95
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 84.05%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        472,265,597,147.95
    Mem Busy                                               8.38
    Max Bandwidth                                         11.93
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           40.53
    Mem Pipes Busy                                         5.02
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.843%                                                                                     
          Out of the 29441440.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.19%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.94
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.06
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.07%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 106.3 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         106.47
    Warp Cycles Per Executed Instruction                       107.17
    Avg. Active Threads Per Warp                                31.86
    Avg. Not Predicated Off Threads Per Warp                    29.61
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.07%                                                                                     
          On average, each warp of this workload spends 93.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.1% of the total average of 106.5 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.66
    Executed Instructions                                     318,292
    Avg. Issued Instructions Per Scheduler                     541.18
    Issued Instructions                                       320,378
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.26
    Achieved Active Warps Per SM                        4.00
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.63%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,117.62
    Total DRAM Elapsed Cycles                      36,700,160
    Average L1 Active Cycles                        57,559.07
    Total L1 Elapsed Cycles                        23,547,148
    Average L2 Active Cycles                       156,290.67
    Total L2 Elapsed Cycles                        30,503,888
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,559.07
    Total SM Elapsed Cycles                        23,547,148
    Average SMSP Active Cycles                      57,538.20
    Total SMSP Elapsed Cycles                      94,188,592
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.84%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.13% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.87%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.24% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.84%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.13% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,642
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.38%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,994,072,914.34
    SM Frequency                        1,121,054,434.13
    Elapsed Cycles                               161,310
    Memory Throughput                              11.92
    DRAM Throughput                                 7.17
    Duration                                     143,072
    L1/TEX Cache Throughput                        33.04
    L2 Cache Throughput                             8.92
    SM Active Cycles                           57,460.49
    Compute (SM) Throughput                         5.77
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 15.98
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 84.02%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        476,594,945,202.42
    Mem Busy                                               8.33
    Max Bandwidth                                         11.92
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           37.03
    Mem Pipes Busy                                         5.02
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.86%                                                                                      
          Out of the 29445184.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.18%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.95
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.05
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.08%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 105.7 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         105.92
    Warp Cycles Per Executed Instruction                       106.62
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    29.60
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 87.99%                                                                                     
          On average, each warp of this workload spends 93.2 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.0% of the total average of 105.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.70
    Executed Instructions                                     318,316
    Avg. Issued Instructions Per Scheduler                     541.25
    Issued Instructions                                       320,420
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.74%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,294.62
    Total DRAM Elapsed Cycles                      36,572,160
    Average L1 Active Cycles                        57,460.49
    Total L1 Elapsed Cycles                        23,571,386
    Average L2 Active Cycles                       156,534.05
    Total L2 Elapsed Cycles                        30,398,180
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,460.49
    Total SM Elapsed Cycles                        23,571,386
    Average SMSP Active Cycles                      57,185.14
    Total SMSP Elapsed Cycles                      94,285,544
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.8%                                                                                      
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.65%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.08% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.8%                                                                                      
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.19% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,621
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.83%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,451,850.20
    SM Frequency                        1,120,914,372.49
    Elapsed Cycles                               161,792
    Memory Throughput                              11.97
    DRAM Throughput                                 7.11
    Duration                                     143,552
    L1/TEX Cache Throughput                        32.96
    L2 Cache Throughput                             8.95
    SM Active Cycles                           57,612.51
    Compute (SM) Throughput                         5.79
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 15.94
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 84.06%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        472,704,413,731.61
    Mem Busy                                               8.37
    Max Bandwidth                                         11.97
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           38.98
    Mem Pipes Busy                                         5.03
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.848%                                                                                     
          Out of the 29444544.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.22%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.94
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.06
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.03%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 106.3 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         106.46
    Warp Cycles Per Executed Instruction                       107.19
    Avg. Active Threads Per Warp                                31.86
    Avg. Not Predicated Off Threads Per Warp                    29.61
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.03%                                                                                     
          On average, each warp of this workload spends 93.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.1% of the total average of 106.5 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.64
    Executed Instructions                                     318,284
    Avg. Issued Instructions Per Scheduler                     541.32
    Issued Instructions                                       320,463
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.25
    Achieved Active Warps Per SM                        4.00
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.65%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,133.62
    Total DRAM Elapsed Cycles                      36,689,152
    Average L1 Active Cycles                        57,612.51
    Total L1 Elapsed Cycles                        23,483,448
    Average L2 Active Cycles                       158,103.46
    Total L2 Elapsed Cycles                        30,491,008
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,612.51
    Total SM Elapsed Cycles                        23,483,448
    Average SMSP Active Cycles                      57,532.71
    Total SMSP Elapsed Cycles                      93,933,792
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.99%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.33% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.83%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 62.96% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.99%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.33% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,651
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 89.45%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,994,370,637.24
    SM Frequency                        1,121,318,924.51
    Elapsed Cycles                               160,120
    Memory Throughput                              11.87
    DRAM Throughput                                 7.21
    Duration                                     142,112
    L1/TEX Cache Throughput                        33.06
    L2 Cache Throughput                             9.03
    SM Active Cycles                           57,433.36
    Compute (SM) Throughput                         5.74
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 15.99
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 84.01%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        479,538,842,603.02
    Mem Busy                                               8.40
    Max Bandwidth                                         11.87
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           38.02
    Mem Pipes Busy                                         5.00
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.892%                                                                                     
          Out of the 29425248.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.13%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.94
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.06
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.13%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 105.8 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         105.64
    Warp Cycles Per Executed Instruction                       106.35
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    29.60
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.13%                                                                                     
          On average, each warp of this workload spends 93.3 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.3% of the total average of 105.6 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.64
    Executed Instructions                                     318,284
    Avg. Issued Instructions Per Scheduler                     541.27
    Issued Instructions                                       320,433
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.22
    Achieved Active Warps Per SM                        3.98
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.81%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,275.50
    Total DRAM Elapsed Cycles                      36,329,472
    Average L1 Active Cycles                        57,433.36
    Total L1 Elapsed Cycles                        23,667,834
    Average L2 Active Cycles                       156,261.65
    Total L2 Elapsed Cycles                        30,194,952
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,433.36
    Total SM Elapsed Cycles                        23,667,834
    Average SMSP Active Cycles                      57,291.95
    Total SMSP Elapsed Cycles                      94,671,336
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.62%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 62.98% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.64%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.62%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 62.98% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,643
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 89.27%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,828,194.82
    SM Frequency                        1,120,645,177.06
    Elapsed Cycles                               161,372
    Memory Throughput                              11.84
    DRAM Throughput                                 7.12
    Duration                                     143,232
    L1/TEX Cache Throughput                        33.04
    L2 Cache Throughput                             9.03
    SM Active Cycles                           57,476.70
    Compute (SM) Throughput                         5.72
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 15.97
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 84.03%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        473,551,385,165.33
    Mem Busy                                               8.38
    Max Bandwidth                                         11.84
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           38.81
    Mem Pipes Busy                                         4.98
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.86%                                                                                      
          Out of the 29441472.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.1%                                                                                      
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.94
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.06
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.16%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 106.2 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         106.28
    Warp Cycles Per Executed Instruction                       106.96
    Avg. Active Threads Per Warp                                31.86
    Avg. Not Predicated Off Threads Per Warp                    29.61
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.16%                                                                                     
          On average, each warp of this workload spends 93.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.2% of the total average of 106.3 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.70
    Executed Instructions                                     318,320
    Avg. Issued Instructions Per Scheduler                     541.14
    Issued Instructions                                       320,353
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.25
    Achieved Active Warps Per SM                        4.00
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.64%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                         33,119
    Total DRAM Elapsed Cycles                      36,610,816
    Average L1 Active Cycles                        57,476.70
    Total L1 Elapsed Cycles                        23,738,508
    Average L2 Active Cycles                       156,991.59
    Total L2 Elapsed Cycles                        30,427,068
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,476.70
    Total SM Elapsed Cycles                        23,738,508
    Average SMSP Active Cycles                      57,468.25
    Total SMSP Elapsed Cycles                      94,954,032
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.64%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.63%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.17% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.64%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.19% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,621
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 89%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,994,553,009.88
    SM Frequency                        1,120,089,110.23
    Elapsed Cycles                               160,480
    Memory Throughput                              11.86
    DRAM Throughput                                 7.19
    Duration                                     142,464
    L1/TEX Cache Throughput                        33.13
    L2 Cache Throughput                             8.97
    SM Active Cycles                           57,308.26
    Compute (SM) Throughput                         5.74
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 16.02
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 83.98%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        478,477,987,421.38
    Mem Busy                                               8.40
    Max Bandwidth                                         11.86
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           37.72
    Mem Pipes Busy                                         4.99
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.882%                                                                                     
          Out of the 29414848.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.12%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.95
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.05
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.14%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 105.7 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         105.79
    Warp Cycles Per Executed Instruction                       106.47
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    29.60
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.14%                                                                                     
          On average, each warp of this workload spends 93.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.3% of the total average of 105.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.68
    Executed Instructions                                     318,308
    Avg. Issued Instructions Per Scheduler                     541.16
    Issued Instructions                                       320,368
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        4.00
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.7%                                                                                      
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,284.12
    Total DRAM Elapsed Cycles                      36,421,120
    Average L1 Active Cycles                        57,308.26
    Total L1 Elapsed Cycles                        23,686,822
    Average L2 Active Cycles                       156,173.82
    Total L2 Elapsed Cycles                        30,265,424
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,308.26
    Total SM Elapsed Cycles                        23,686,822
    Average SMSP Active Cycles                      57,215.61
    Total SMSP Elapsed Cycles                      94,747,288
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.59%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.08% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.56%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.59%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.08% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,616
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 89.01%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,807,700.97
    SM Frequency                        1,120,850,983.73
    Elapsed Cycles                               160,158
    Memory Throughput                              11.85
    DRAM Throughput                                 7.18
    Duration                                     142,112
    L1/TEX Cache Throughput                        33.00
    L2 Cache Throughput                             9.09
    SM Active Cycles                           57,546.37
    Compute (SM) Throughput                         5.73
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 15.96
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 84.04%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        477,369,961,720.33
    Mem Busy                                               8.47
    Max Bandwidth                                         11.85
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           38.96
    Mem Pipes Busy                                         4.98
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.9%                                                                                       
          Out of the 29443776.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.11%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.94
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.06
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.15%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 106.1 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         106.09
    Warp Cycles Per Executed Instruction                       106.80
    Avg. Active Threads Per Warp                                31.86
    Avg. Not Predicated Off Threads Per Warp                    29.61
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.1%                                                                                      
          On average, each warp of this workload spends 93.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.1% of the total average of 106.1 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.64
    Executed Instructions                                     318,284
    Avg. Issued Instructions Per Scheduler                     541.21
    Issued Instructions                                       320,396
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.74%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                         33,125
    Total DRAM Elapsed Cycles                      36,324,352
    Average L1 Active Cycles                        57,546.37
    Total L1 Elapsed Cycles                        23,722,326
    Average L2 Active Cycles                       157,115.41
    Total L2 Elapsed Cycles                        30,192,008
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,546.37
    Total SM Elapsed Cycles                        23,722,326
    Average SMSP Active Cycles                      57,436.36
    Total SMSP Elapsed Cycles                      94,889,304
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.66%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.11% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.55%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 62.93% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.66%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.11% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,621
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 89.77%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,994,240,278.71
    SM Frequency                        1,120,609,090.53
    Elapsed Cycles                               160,379
    Memory Throughput                              11.88
    DRAM Throughput                                 7.20
    Duration                                     142,368
    L1/TEX Cache Throughput                        33.11
    L2 Cache Throughput                             9.02
    SM Active Cycles                           57,348.41
    Compute (SM) Throughput                         5.75
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 16.01
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 83.99%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        478,687,345,470.89
    Mem Busy                                               8.48
    Max Bandwidth                                         11.88
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           37.84
    Mem Pipes Busy                                         5.00
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.88%                                                                                      
          Out of the 29445248.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.14%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.95
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.05
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.12%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 105.7 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         105.74
    Warp Cycles Per Executed Instruction                       106.44
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    29.60
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.12%                                                                                     
          On average, each warp of this workload spends 93.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.3% of the total average of 105.7 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.66
    Executed Instructions                                     318,296
    Avg. Issued Instructions Per Scheduler                     541.22
    Issued Instructions                                       320,403
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.73%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,276.25
    Total DRAM Elapsed Cycles                      36,393,728
    Average L1 Active Cycles                        57,348.41
    Total L1 Elapsed Cycles                        23,646,828
    Average L2 Active Cycles                       156,030.59
    Total L2 Elapsed Cycles                        30,247,300
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,348.41
    Total SM Elapsed Cycles                        23,646,828
    Average SMSP Active Cycles                      57,222.42
    Total SMSP Elapsed Cycles                      94,587,312
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.66%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.13% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.6%                                                                                      
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.66%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.13% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,619
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.98%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,371,630.58
    SM Frequency                        1,121,709,118.15
    Elapsed Cycles                               163,237
    Memory Throughput                              11.91
    DRAM Throughput                                 7.05
    Duration                                     144,832
    L1/TEX Cache Throughput                        32.95
    L2 Cache Throughput                             8.88
    SM Active Cycles                           57,621.46
    Compute (SM) Throughput                         5.76
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 15.93
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 84.07%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        468,798,939,460.89
    Mem Busy                                               8.30
    Max Bandwidth                                         11.91
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           38.14
    Mem Pipes Busy                                         5.01
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.804%                                                                                     
          Out of the 29441728.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.16%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.94
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.06
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.09%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 106.2 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         106.13
    Warp Cycles Per Executed Instruction                       106.83
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    29.60
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.09%                                                                                     
          On average, each warp of this workload spends 93.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.2% of the total average of 106.1 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.72
    Executed Instructions                                     318,332
    Avg. Issued Instructions Per Scheduler                     541.29
    Issued Instructions                                       320,444
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.23
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.76%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,152.88
    Total DRAM Elapsed Cycles                      37,015,552
    Average L1 Active Cycles                        57,621.46
    Total L1 Elapsed Cycles                        23,598,436
    Average L2 Active Cycles                       156,933.20
    Total L2 Elapsed Cycles                        30,756,888
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,621.46
    Total SM Elapsed Cycles                        23,598,436
    Average SMSP Active Cycles                      57,489.82
    Total SMSP Elapsed Cycles                      94,393,744
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.86%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.24% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.74%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.07% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.86%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.24% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,628
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.02%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,748,604.60
    SM Frequency                        1,120,310,581.32
    Elapsed Cycles                               161,453
    Memory Throughput                              11.93
    DRAM Throughput                                 7.15
    Duration                                     143,328
    L1/TEX Cache Throughput                        33.11
    L2 Cache Throughput                             8.97
    SM Active Cycles                           57,344.80
    Compute (SM) Throughput                         5.77
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 16.01
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 83.99%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        475,772,270,596.12
    Mem Busy                                               8.33
    Max Bandwidth                                         11.93
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           39.00
    Mem Pipes Busy                                         5.02
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.855%                                                                                     
          Out of the 29441440.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.18%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.94
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.06
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.07%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 105.9 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         105.79
    Warp Cycles Per Executed Instruction                       106.47
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    29.60
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.07%                                                                                     
          On average, each warp of this workload spends 93.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.3% of the total average of 105.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.69
    Executed Instructions                                     318,312
    Avg. Issued Instructions Per Scheduler                     541.18
    Issued Instructions                                       320,376
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.72%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,296.62
    Total DRAM Elapsed Cycles                      36,634,624
    Average L1 Active Cycles                        57,344.80
    Total L1 Elapsed Cycles                        23,559,686
    Average L2 Active Cycles                       156,084.36
    Total L2 Elapsed Cycles                        30,449,240
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,344.80
    Total SM Elapsed Cycles                        23,559,686
    Average SMSP Active Cycles                      57,285.89
    Total SMSP Elapsed Cycles                      94,238,744
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.8%                                                                                      
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.28% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.68%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.03% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.8%                                                                                      
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.28% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,625
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.42%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,754,163.89
    SM Frequency                        1,121,010,472.19
    Elapsed Cycles                               162,413
    Memory Throughput                              11.97
    DRAM Throughput                                 7.08
    Duration                                     144,096
    L1/TEX Cache Throughput                        33.01
    L2 Cache Throughput                             8.98
    SM Active Cycles                           57,514.09
    Compute (SM) Throughput                         5.79
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 15.97
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 84.03%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        470,708,416,611.15
    Mem Busy                                               8.31
    Max Bandwidth                                         11.97
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           39.04
    Mem Pipes Busy                                         5.04
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.832%                                                                                     
          Out of the 29443488.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.22%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.94
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.06
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.03%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 106.3 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         106.08
    Warp Cycles Per Executed Instruction                       106.78
    Avg. Active Threads Per Warp                                31.86
    Avg. Not Predicated Off Threads Per Warp                    29.61
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.03%                                                                                     
          On average, each warp of this workload spends 93.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.2% of the total average of 106.1 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.69
    Executed Instructions                                     318,312
    Avg. Issued Instructions Per Scheduler                     541.27
    Issued Instructions                                       320,430
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.72%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,118.75
    Total DRAM Elapsed Cycles                      36,830,976
    Average L1 Active Cycles                        57,514.09
    Total L1 Elapsed Cycles                        23,479,740
    Average L2 Active Cycles                       156,892.98
    Total L2 Elapsed Cycles                        30,612,356
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,514.09
    Total SM Elapsed Cycles                        23,479,740
    Average SMSP Active Cycles                      57,554.24
    Total SMSP Elapsed Cycles                      93,918,960
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.9%                                                                                      
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.16% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.78%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 62.80% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.9%                                                                                      
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.16% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,678
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.41%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,994,346,171.07
    SM Frequency                        1,121,333,407.97
    Elapsed Cycles                               161,095
    Memory Throughput                              11.93
    DRAM Throughput                                 7.17
    Duration                                     142,912
    L1/TEX Cache Throughput                        33.12
    L2 Cache Throughput                             8.98
    SM Active Cycles                           57,322.62
    Compute (SM) Throughput                         5.77
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 16.02
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 83.98%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        477,180,474,697.72
    Mem Busy                                               8.37
    Max Bandwidth                                         11.93
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           38.07
    Mem Pipes Busy                                         5.02
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.866%                                                                                     
          Out of the 29415360.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.18%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.95
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.05
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.07%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 105.6 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         105.73
    Warp Cycles Per Executed Instruction                       106.43
    Avg. Active Threads Per Warp                                31.86
    Avg. Not Predicated Off Threads Per Warp                    29.60
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.07%                                                                                     
          On average, each warp of this workload spends 93.2 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.2% of the total average of 105.7 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.65
    Executed Instructions                                     318,288
    Avg. Issued Instructions Per Scheduler                     541.21
    Issued Instructions                                       320,396
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.72%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,298.25
    Total DRAM Elapsed Cycles                      36,533,760
    Average L1 Active Cycles                        57,322.62
    Total L1 Elapsed Cycles                        23,558,576
    Average L2 Active Cycles                       156,419.18
    Total L2 Elapsed Cycles                        30,362,392
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,322.62
    Total SM Elapsed Cycles                        23,558,576
    Average SMSP Active Cycles                      57,133.66
    Total SMSP Elapsed Cycles                      94,234,304
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.74%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.15% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.65%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.11% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.74%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.15% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,600
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.87%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,992,972,222.22
    SM Frequency                        1,121,364,583.33
    Elapsed Cycles                               162,286
    Memory Throughput                              11.81
    DRAM Throughput                                 7.09
    Duration                                     144,000
    L1/TEX Cache Throughput                        32.98
    L2 Cache Throughput                             9.01
    SM Active Cycles                           57,569.39
    Compute (SM) Throughput                         5.71
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                 15.95
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 84.05%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ---------------
    Metric Name                  Metric Unit    Metric Value
    ---------------------------- ----------- ---------------
    Memory Throughput                        471,232,000,000
    Mem Busy                                            8.32
    Max Bandwidth                                      11.81
    L1/TEX Hit Rate                                    98.44
    L2 Compression Success Rate                      (!) n/a
    L2 Compression Input Sectors                     (!) n/a
    L2 Hit Rate                                        38.37
    Mem Pipes Busy                                      4.97
    ---------------------------- ----------- ---------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.827%                                                                                     
          Out of the 29441600.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.07%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.94
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.06
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.19%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 106.1 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         106.21
    Warp Cycles Per Executed Instruction                       106.92
    Avg. Active Threads Per Warp                                31.86
    Avg. Not Predicated Off Threads Per Warp                    29.61
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.19%                                                                                     
          On average, each warp of this workload spends 93.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.2% of the total average of 106.2 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.67
    Executed Instructions                                     318,300
    Avg. Issued Instructions Per Scheduler                     541.26
    Issued Instructions                                       320,425
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.71%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,133.50
    Total DRAM Elapsed Cycles                      36,799,232
    Average L1 Active Cycles                        57,569.39
    Total L1 Elapsed Cycles                        23,795,400
    Average L2 Active Cycles                       157,264.62
    Total L2 Elapsed Cycles                        30,584,848
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,569.39
    Total SM Elapsed Cycles                        23,795,400
    Average SMSP Active Cycles                      57,407.60
    Total SMSP Elapsed Cycles                      95,181,600
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.55%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 62.99% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.53%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.55%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 62.99% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,610
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.7%                                                                                      
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,992,874,775.58
    SM Frequency                        1,120,872,839.99
    Elapsed Cycles                               160,672
    Memory Throughput                              11.86
    DRAM Throughput                                 7.19
    Duration                                     142,592
    L1/TEX Cache Throughput                        33.17
    L2 Cache Throughput                             8.97
    SM Active Cycles                           57,251.53
    Compute (SM) Throughput                         5.74
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.95
    Issued Ipc Active                        0.04
    SM Busy                                 16.04
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 83.96%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        478,314,183,123.88
    Mem Busy                                               8.40
    Max Bandwidth                                         11.86
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           39.57
    Mem Pipes Busy                                         4.99
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 4.848%                                                                                     
          Out of the 29441568.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 11.12%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.94
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.06
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.14%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 106.0 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         105.73
    Warp Cycles Per Executed Instruction                       106.41
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    29.60
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 88.14%                                                                                     
          On average, each warp of this workload spends 93.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 88.6% of the total average of 105.7 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   537.70
    Executed Instructions                                     318,320
    Avg. Issued Instructions Per Scheduler                     541.18
    Issued Instructions                                       320,376
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.25
    Achieved Active Warps Per SM                        4.00
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.68%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                      33,302.62
    Total DRAM Elapsed Cycles                      36,438,528
    Average L1 Active Cycles                        57,251.53
    Total L1 Elapsed Cycles                        23,688,010
    Average L2 Active Cycles                       156,629.34
    Total L2 Elapsed Cycles                        30,285,756
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        57,251.53
    Total SM Elapsed Cycles                        23,688,010
    Average SMSP Active Cycles                      57,350.34
    Total SMSP Elapsed Cycles                      94,752,040
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 22.57%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.11% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.64%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 63.17% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 22.57%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.11% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.06
    Branch Instructions                         18,656
    Branch Efficiency                            99.80
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 89.21%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,992,697,768.76
    SM Frequency                        1,126,942,824.54
    Elapsed Cycles                                89,185
    Memory Throughput                               9.66
    DRAM Throughput                                 3.25
    Duration                                      78,880
    L1/TEX Cache Throughput                        47.53
    L2 Cache Throughput                             7.62
    SM Active Cycles                           17,966.24
    Compute (SM) Throughput                         2.60
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.78
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.22%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        216,097,363,083.16
    Mem Busy                                               6.82
    Max Bandwidth                                          9.66
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           43.22
    Mem Pipes Busy                                         2.27
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.925%                                                                                     
          Out of the 16862112.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 9.053%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.34%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 94.9 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          95.14
    Warp Cycles Per Executed Instruction                        95.56
    Avg. Active Threads Per Warp                                31.75
    Avg. Not Predicated Off Threads Per Warp                    29.69
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.56%                                                                                     
          On average, each warp of this workload spends 79.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.6% of the total average of 95.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.53
    Executed Instructions                                     111,020
    Avg. Issued Instructions Per Scheduler                     188.38
    Issued Instructions                                       111,520
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.74%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,323.12
    Total DRAM Elapsed Cycles                      20,156,416
    Average L1 Active Cycles                        17,966.24
    Total L1 Elapsed Cycles                        13,089,324
    Average L2 Active Cycles                        83,889.32
    Total L2 Elapsed Cycles                        16,754,396
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,966.24
    Total SM Elapsed Cycles                        13,089,324
    Average SMSP Active Cycles                      17,870.18
    Total SMSP Elapsed Cycles                      52,357,296
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.99%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.72% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.9%                                                                                      
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.67% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.99%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.72% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,738
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 86.37%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,991,806,636.62
    SM Frequency                        1,126,044,973.88
    Elapsed Cycles                                88,268
    Memory Throughput                               9.52
    DRAM Throughput                                 3.28
    Duration                                      78,112
    L1/TEX Cache Throughput                        47.73
    L2 Cache Throughput                             7.68
    SM Active Cycles                           17,893.78
    Compute (SM) Throughput                         2.56
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.83
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.17%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        218,218,762,802.13
    Mem Busy                                               6.73
    Max Bandwidth                                          9.52
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           43.87
    Mem Pipes Busy                                         2.23
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.936%                                                                                     
          Out of the 16831936.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 8.924%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.48%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 94.9 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          95.34
    Warp Cycles Per Executed Instruction                        95.73
    Avg. Active Threads Per Warp                                31.76
    Avg. Not Predicated Off Threads Per Warp                    29.71
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.71%                                                                                     
          On average, each warp of this workload spends 79.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.7% of the total average of 95.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.53
    Executed Instructions                                     111,016
    Avg. Issued Instructions Per Scheduler                     188.29
    Issued Instructions                                       111,466
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.27
    Achieved Active Warps Per SM                        4.01
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.55%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,323
    Total DRAM Elapsed Cycles                      19,955,712
    Average L1 Active Cycles                        17,893.78
    Total L1 Elapsed Cycles                        13,278,088
    Average L2 Active Cycles                        81,500.86
    Total L2 Elapsed Cycles                        16,584,748
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,893.78
    Total SM Elapsed Cycles                        13,278,088
    Average SMSP Active Cycles                      17,868.14
    Total SMSP Elapsed Cycles                      53,112,352
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.71%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.75% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.68%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.74% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.71%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.75% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,742
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 84.77%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,992,554,700.16
    SM Frequency                        1,127,502,342.48
    Elapsed Cycles                                89,316
    Memory Throughput                               9.58
    DRAM Throughput                                 3.25
    Duration                                      78,976
    L1/TEX Cache Throughput                        47.76
    L2 Cache Throughput                             7.63
    SM Active Cycles                           17,882.61
    Compute (SM) Throughput                         2.58
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.84
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.16%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        215,831,442,463.53
    Mem Busy                                               6.80
    Max Bandwidth                                          9.58
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           44.17
    Mem Pipes Busy                                         2.25
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.927%                                                                                     
          Out of the 16831936.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 8.983%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.42%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 95.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          95.07
    Warp Cycles Per Executed Instruction                        95.45
    Avg. Active Threads Per Warp                                31.77
    Avg. Not Predicated Off Threads Per Warp                    29.72
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.78%                                                                                     
          On average, each warp of this workload spends 79.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.8% of the total average of 95.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.53
    Executed Instructions                                     111,020
    Avg. Issued Instructions Per Scheduler                     188.28
    Issued Instructions                                       111,463
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.26
    Achieved Active Warps Per SM                        4.00
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.63%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,323
    Total DRAM Elapsed Cycles                      20,180,224
    Average L1 Active Cycles                        17,882.61
    Total L1 Elapsed Cycles                        13,190,052
    Average L2 Active Cycles                        83,623.18
    Total L2 Elapsed Cycles                        16,776,844
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,882.61
    Total SM Elapsed Cycles                        13,190,052
    Average SMSP Active Cycles                      17,891.49
    Total SMSP Elapsed Cycles                      52,760,208
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.78%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.63% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.82%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.78% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.78%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.63% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,736
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 85.98%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,992,942,729.49
    SM Frequency                        1,127,141,932.37
    Elapsed Cycles                                89,103
    Memory Throughput                               9.65
    DRAM Throughput                                 3.25
    Duration                                      78,784
    L1/TEX Cache Throughput                        47.68
    L2 Cache Throughput                             7.56
    SM Active Cycles                           17,909.34
    Compute (SM) Throughput                         2.59
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.82
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.18%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        216,357,432,981.32
    Mem Busy                                               6.91
    Max Bandwidth                                          9.65
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           44.30
    Mem Pipes Busy                                         2.26
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.936%                                                                                     
          Out of the 16832032.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 9.045%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.35%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 95.2 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          94.94
    Warp Cycles Per Executed Instruction                        95.30
    Avg. Active Threads Per Warp                                31.78
    Avg. Not Predicated Off Threads Per Warp                    29.73
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.9%                                                                                      
          On average, each warp of this workload spends 79.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.9% of the total average of 94.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.49
    Executed Instructions                                     110,996
    Avg. Issued Instructions Per Scheduler                     188.21
    Issued Instructions                                       111,421
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.73%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,323
    Total DRAM Elapsed Cycles                      20,133,120
    Average L1 Active Cycles                        17,909.34
    Total L1 Elapsed Cycles                        13,100,396
    Average L2 Active Cycles                        82,943.89
    Total L2 Elapsed Cycles                        16,734,800
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,909.34
    Total SM Elapsed Cycles                        13,100,396
    Average SMSP Active Cycles                      17,910.90
    Total SMSP Elapsed Cycles                      52,401,584
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.91%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.65% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.94%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.78% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.91%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.65% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,748
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 85.5%                                                                                      
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,992,857,142.86
    SM Frequency                        1,126,833,545.92
    Elapsed Cycles                                88,619
    Memory Throughput                               9.53
    DRAM Throughput                                 3.27
    Duration                                      78,400
    L1/TEX Cache Throughput                        47.68
    L2 Cache Throughput                             7.71
    SM Active Cycles                           17,910.80
    Compute (SM) Throughput                         2.56
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.82
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.18%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        217,417,142,857.14
    Mem Busy                                               6.72
    Max Bandwidth                                          9.53
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           43.79
    Mem Pipes Busy                                         2.24
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.941%                                                                                     
          Out of the 16835520.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 8.938%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.47%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 95.3 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          94.87
    Warp Cycles Per Executed Instruction                        95.35
    Avg. Active Threads Per Warp                                31.77
    Avg. Not Predicated Off Threads Per Warp                    29.73
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.6%                                                                                      
          On average, each warp of this workload spends 79.3 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.6% of the total average of 94.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.39
    Executed Instructions                                     110,936
    Avg. Issued Instructions Per Scheduler                     188.34
    Issued Instructions                                       111,499
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.74%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,323
    Total DRAM Elapsed Cycles                      20,034,560
    Average L1 Active Cycles                        17,910.80
    Total L1 Elapsed Cycles                        13,256,448
    Average L2 Active Cycles                        83,043.14
    Total L2 Elapsed Cycles                        16,654,208
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,910.80
    Total SM Elapsed Cycles                        13,256,448
    Average SMSP Active Cycles                      17,949.79
    Total SMSP Elapsed Cycles                      53,025,792
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.73%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.65% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.79%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.79% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.73%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.65% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,734
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 86.01%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,994,006,501.42
    SM Frequency                        1,127,398,351.79
    Elapsed Cycles                                89,058
    Memory Throughput                               9.63
    DRAM Throughput                                 3.25
    Duration                                      78,752
    L1/TEX Cache Throughput                        47.69
    L2 Cache Throughput                             7.66
    SM Active Cycles                           17,908.46
    Compute (SM) Throughput                         2.59
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.82
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.18%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        216,445,347,419.75
    Mem Busy                                               6.92
    Max Bandwidth                                          9.63
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           44.93
    Mem Pipes Busy                                         2.26
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.931%                                                                                     
          Out of the 16833984.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 9.03%                                                                                      
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.37%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 94.9 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          94.73
    Warp Cycles Per Executed Instruction                        95.09
    Avg. Active Threads Per Warp                                31.74
    Avg. Not Predicated Off Threads Per Warp                    29.70
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 84.04%                                                                                     
          On average, each warp of this workload spends 79.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 84.0% of the total average of 94.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.63
    Executed Instructions                                     111,076
    Avg. Issued Instructions Per Scheduler                     188.34
    Issued Instructions                                       111,497
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.23
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.78%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,323
    Total DRAM Elapsed Cycles                      20,130,304
    Average L1 Active Cycles                        17,908.46
    Total L1 Elapsed Cycles                        13,122,704
    Average L2 Active Cycles                        83,258.65
    Total L2 Elapsed Cycles                        16,728,636
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,908.46
    Total SM Elapsed Cycles                        13,122,704
    Average SMSP Active Cycles                      17,882.69
    Total SMSP Elapsed Cycles                      52,490,816
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.89%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.68% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.88%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.74% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.89%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.68% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,724
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 85.85%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,310,866.01
    SM Frequency                        1,126,638,774.00
    Elapsed Cycles                                88,553
    Memory Throughput                               9.63
    DRAM Throughput                                 3.27
    Duration                                      78,336
    L1/TEX Cache Throughput                        47.63
    L2 Cache Throughput                             7.68
    SM Active Cycles                           17,931.03
    Compute (SM) Throughput                         2.59
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.80
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.2%                                                                                      
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        217,617,647,058.82
    Mem Busy                                               6.97
    Max Bandwidth                                          9.63
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           45.03
    Mem Pipes Busy                                         2.26
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.942%                                                                                     
          Out of the 16831936.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 9.029%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.37%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 95.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          94.79
    Warp Cycles Per Executed Instruction                        95.14
    Avg. Active Threads Per Warp                                31.74
    Avg. Not Predicated Off Threads Per Warp                    29.70
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.73%                                                                                     
          On average, each warp of this workload spends 79.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.7% of the total average of 94.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.57
    Executed Instructions                                     111,044
    Avg. Issued Instructions Per Scheduler                     188.28
    Issued Instructions                                       111,463
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.22
    Achieved Active Warps Per SM                        3.98
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.82%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,323.88
    Total DRAM Elapsed Cycles                      20,020,480
    Average L1 Active Cycles                        17,931.03
    Total L1 Elapsed Cycles                        13,124,190
    Average L2 Active Cycles                        82,387.48
    Total L2 Elapsed Cycles                        16,640,684
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,931.03
    Total SM Elapsed Cycles                        13,124,190
    Average SMSP Active Cycles                      17,901.37
    Total SMSP Elapsed Cycles                      52,496,760
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.92%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.74% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.88%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.67% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.92%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.74% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,740
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 85.4%                                                                                      
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,991,932,189.54
    SM Frequency                        1,126,010,072.00
    Elapsed Cycles                                88,513
    Memory Throughput                               9.56
    DRAM Throughput                                 3.27
    Duration                                      78,336
    L1/TEX Cache Throughput                        47.55
    L2 Cache Throughput                             7.69
    SM Active Cycles                           17,960.33
    Compute (SM) Throughput                         2.57
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.78
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.22%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        217,598,039,215.69
    Mem Busy                                               6.79
    Max Bandwidth                                          9.56
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           45.85
    Mem Pipes Busy                                         2.24
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.932%                                                                                     
          Out of the 16831936.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 8.961%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.44%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 94.8 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          94.89
    Warp Cycles Per Executed Instruction                        95.29
    Avg. Active Threads Per Warp                                31.77
    Avg. Not Predicated Off Threads Per Warp                    29.72
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 84.05%                                                                                     
          On average, each warp of this workload spends 79.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 84.0% of the total average of 94.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.53
    Executed Instructions                                     111,016
    Avg. Issued Instructions Per Scheduler                     188.32
    Issued Instructions                                       111,484
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.22
    Achieved Active Warps Per SM                        3.98
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.83%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,323.12
    Total DRAM Elapsed Cycles                      20,013,568
    Average L1 Active Cycles                        17,960.33
    Total L1 Elapsed Cycles                        13,222,906
    Average L2 Active Cycles                        82,585.27
    Total L2 Elapsed Cycles                        16,634,428
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,960.33
    Total SM Elapsed Cycles                        13,222,906
    Average SMSP Active Cycles                      17,861.43
    Total SMSP Elapsed Cycles                      52,891,624
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.81%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.67% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.75%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.79% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.81%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.67% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,734
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 85.64%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,007,701.66
    SM Frequency                        1,126,892,164.07
    Elapsed Cycles                                89,214
    Memory Throughput                               9.67
    DRAM Throughput                                 3.25
    Duration                                      78,944
    L1/TEX Cache Throughput                        47.68
    L2 Cache Throughput                             7.58
    SM Active Cycles                           17,909.10
    Compute (SM) Throughput                         2.60
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.82
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.18%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        215,918,929,874.34
    Mem Busy                                               6.85
    Max Bandwidth                                          9.67
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           44.53
    Mem Pipes Busy                                         2.27
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.925%                                                                                     
          Out of the 16831872.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 9.064%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.33%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 95.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          94.89
    Warp Cycles Per Executed Instruction                        95.24
    Avg. Active Threads Per Warp                                31.75
    Avg. Not Predicated Off Threads Per Warp                    29.71
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.66%                                                                                     
          On average, each warp of this workload spends 79.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.7% of the total average of 94.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.59
    Executed Instructions                                     111,056
    Avg. Issued Instructions Per Scheduler                     188.29
    Issued Instructions                                       111,465
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.74%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,323
    Total DRAM Elapsed Cycles                      20,174,336
    Average L1 Active Cycles                        17,909.10
    Total L1 Elapsed Cycles                        13,072,304
    Average L2 Active Cycles                        83,741.21
    Total L2 Elapsed Cycles                        16,770,772
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,909.10
    Total SM Elapsed Cycles                        13,072,304
    Average SMSP Active Cycles                      17,887.87
    Total SMSP Elapsed Cycles                      52,289,216
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.97%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.76% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.94%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.70% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.97%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.76% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,722
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 86.13%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,990,658,001.62
    SM Frequency                        1,126,291,505.89
    Elapsed Cycles                                89,043
    Memory Throughput                               9.52
    DRAM Throughput                                 3.26
    Duration                                      78,784
    L1/TEX Cache Throughput                        47.44
    L2 Cache Throughput                             7.66
    SM Active Cycles                           18,001.32
    Compute (SM) Throughput                         2.56
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.75
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.25%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        216,357,432,981.32
    Mem Busy                                               6.72
    Max Bandwidth                                          9.52
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           46.04
    Mem Pipes Busy                                         2.24
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.925%                                                                                     
          Out of the 16833984.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 8.93%                                                                                      
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.48%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 94.8 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          95.20
    Warp Cycles Per Executed Instruction                        95.62
    Avg. Active Threads Per Warp                                31.73
    Avg. Not Predicated Off Threads Per Warp                    29.70
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.35%                                                                                     
          On average, each warp of this workload spends 79.3 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.3% of the total average of 95.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.61
    Executed Instructions                                     111,068
    Avg. Issued Instructions Per Scheduler                     188.45
    Issued Instructions                                       111,562
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.23
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.77%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,323
    Total DRAM Elapsed Cycles                      20,121,600
    Average L1 Active Cycles                        18,001.32
    Total L1 Elapsed Cycles                        13,269,590
    Average L2 Active Cycles                        83,369.90
    Total L2 Elapsed Cycles                        16,727,072
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        18,001.32
    Total SM Elapsed Cycles                        13,269,590
    Average SMSP Active Cycles                      17,874.23
    Total SMSP Elapsed Cycles                      53,078,360
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.79%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.67% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.71%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.80% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.79%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.67% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,728
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 85.98%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,992,706,645.06
    SM Frequency                        1,126,631,824.86
    Elapsed Cycles                                89,287
    Memory Throughput                               9.55
    DRAM Throughput                                 3.25
    Duration                                      78,976
    L1/TEX Cache Throughput                        47.69
    L2 Cache Throughput                             7.64
    SM Active Cycles                           17,906.98
    Compute (SM) Throughput                         2.57
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.82
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.18%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        215,834,683,954.62
    Mem Busy                                               6.82
    Max Bandwidth                                          9.55
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           44.88
    Mem Pipes Busy                                         2.24
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.925%                                                                                     
          Out of the 16836960.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 8.954%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.45%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 95.2 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          95.10
    Warp Cycles Per Executed Instruction                        95.49
    Avg. Active Threads Per Warp                                31.75
    Avg. Not Predicated Off Threads Per Warp                    29.71
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.74%                                                                                     
          On average, each warp of this workload spends 79.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.7% of the total average of 95.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.52
    Executed Instructions                                     111,012
    Avg. Issued Instructions Per Scheduler                     188.27
    Issued Instructions                                       111,457
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.25
    Achieved Active Warps Per SM                        4.00
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.66%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,323.12
    Total DRAM Elapsed Cycles                      20,180,992
    Average L1 Active Cycles                        17,906.98
    Total L1 Elapsed Cycles                        13,233,164
    Average L2 Active Cycles                        83,721.09
    Total L2 Elapsed Cycles                        16,776,660
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,906.98
    Total SM Elapsed Cycles                        13,233,164
    Average SMSP Active Cycles                      17,917.87
    Total SMSP Elapsed Cycles                      52,932,656
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.78%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.79% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.78%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.73% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.78%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.79% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,758
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 86.08%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,992,575,264.44
    SM Frequency                        1,126,876,843.47
    Elapsed Cycles                                88,878
    Memory Throughput                               9.66
    DRAM Throughput                                 3.26
    Duration                                      78,656
    L1/TEX Cache Throughput                        47.55
    L2 Cache Throughput                             7.69
    SM Active Cycles                           17,961.54
    Compute (SM) Throughput                         2.60
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.78
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.22%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        216,709,519,934.91
    Mem Busy                                               6.96
    Max Bandwidth                                          9.66
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           44.91
    Mem Pipes Busy                                         2.27
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.93%                                                                                      
          Out of the 16831936.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 9.059%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.34%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 95.3 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          95.25
    Warp Cycles Per Executed Instruction                        95.64
    Avg. Active Threads Per Warp                                31.77
    Avg. Not Predicated Off Threads Per Warp                    29.72
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.56%                                                                                     
          On average, each warp of this workload spends 79.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.6% of the total average of 95.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.59
    Executed Instructions                                     111,056
    Avg. Issued Instructions Per Scheduler                     188.37
    Issued Instructions                                       111,516
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        4.00
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.7%                                                                                      
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,323
    Total DRAM Elapsed Cycles                      20,098,560
    Average L1 Active Cycles                        17,961.54
    Total L1 Elapsed Cycles                        13,079,424
    Average L2 Active Cycles                        82,246.08
    Total L2 Elapsed Cycles                        16,701,128
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,961.54
    Total SM Elapsed Cycles                        13,079,424
    Average SMSP Active Cycles                      17,959.60
    Total SMSP Elapsed Cycles                      52,317,696
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.99%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.67% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 16.02%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.81% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.99%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.67% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,758
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 84.95%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,991,177,070.58
    SM Frequency                        1,126,311,645.76
    Elapsed Cycles                                88,616
    Memory Throughput                               9.61
    DRAM Throughput                                 3.27
    Duration                                      78,432
    L1/TEX Cache Throughput                        47.64
    L2 Cache Throughput                             7.71
    SM Active Cycles                           17,925.42
    Compute (SM) Throughput                         2.58
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.80
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.2%                                                                                      
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        217,328,437,372.50
    Mem Busy                                               6.85
    Max Bandwidth                                          9.61
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           41.74
    Mem Pipes Busy                                         2.26
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.936%                                                                                     
          Out of the 16835904.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 9.011%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.39%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 95.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          95.12
    Warp Cycles Per Executed Instruction                        95.47
    Avg. Active Threads Per Warp                                31.75
    Avg. Not Predicated Off Threads Per Warp                    29.70
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.35%                                                                                     
          On average, each warp of this workload spends 79.3 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.4% of the total average of 95.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.56
    Executed Instructions                                     111,036
    Avg. Issued Instructions Per Scheduler                     188.27
    Issued Instructions                                       111,453
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        4.00
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.69%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,323
    Total DRAM Elapsed Cycles                      20,034,304
    Average L1 Active Cycles                        17,925.42
    Total L1 Elapsed Cycles                        13,149,338
    Average L2 Active Cycles                        83,386.68
    Total L2 Elapsed Cycles                        16,653,840
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,925.42
    Total SM Elapsed Cycles                        13,149,338
    Average SMSP Active Cycles                      17,883.31
    Total SMSP Elapsed Cycles                      52,597,352
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.88%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.70% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.85%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.75% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.88%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.70% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,712
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 86.37%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,021,597.39
    SM Frequency                        1,126,943,574.27
    Elapsed Cycles                                88,767
    Memory Throughput                               9.67
    DRAM Throughput                                 3.26
    Duration                                      78,528
    L1/TEX Cache Throughput                        47.47
    L2 Cache Throughput                             7.65
    SM Active Cycles                           17,990.14
    Compute (SM) Throughput                         2.60
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.76
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.24%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        217,062,754,686.23
    Mem Busy                                               6.83
    Max Bandwidth                                          9.67
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           44.69
    Mem Pipes Busy                                         2.27
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.95%                                                                                      
          Out of the 16835520.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 9.065%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.33%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 95.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          95.24
    Warp Cycles Per Executed Instruction                        95.57
    Avg. Active Threads Per Warp                                31.76
    Avg. Not Predicated Off Threads Per Warp                    29.71
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.52%                                                                                     
          On average, each warp of this workload spends 79.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.5% of the total average of 95.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.60
    Executed Instructions                                     111,060
    Avg. Issued Instructions Per Scheduler                     188.25
    Issued Instructions                                       111,445
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.23
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.77%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,323
    Total DRAM Elapsed Cycles                      20,068,096
    Average L1 Active Cycles                        17,990.14
    Total L1 Elapsed Cycles                        13,070,986
    Average L2 Active Cycles                        83,289.15
    Total L2 Elapsed Cycles                        16,680,612
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,990.14
    Total SM Elapsed Cycles                        13,070,986
    Average SMSP Active Cycles                      17,895.51
    Total SMSP Elapsed Cycles                      52,283,944
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 16.04%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.72% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.95%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.72% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 16.04%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.72% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,734
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 86.13%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,990,171,990.17
    SM Frequency                        1,125,836,596.54
    Elapsed Cycles                                88,250
    Memory Throughput                               9.69
    DRAM Throughput                                 3.28
    Duration                                      78,144
    L1/TEX Cache Throughput                        47.51
    L2 Cache Throughput                             7.64
    SM Active Cycles                           17,974.45
    Compute (SM) Throughput                         2.61
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.77
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.23%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        218,129,402,129.40
    Mem Busy                                               6.89
    Max Bandwidth                                          9.69
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           43.86
    Mem Pipes Busy                                         2.28
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.956%                                                                                     
          Out of the 16832064.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 9.089%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.31%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 94.9 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          94.81
    Warp Cycles Per Executed Instruction                        95.20
    Avg. Active Threads Per Warp                                31.75
    Avg. Not Predicated Off Threads Per Warp                    29.71
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.78%                                                                                     
          On average, each warp of this workload spends 79.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.8% of the total average of 94.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.53
    Executed Instructions                                     111,020
    Avg. Issued Instructions Per Scheduler                     188.31
    Issued Instructions                                       111,478
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.21
    Achieved Active Warps Per SM                        3.97
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.88%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,323
    Total DRAM Elapsed Cycles                      19,955,712
    Average L1 Active Cycles                        17,974.45
    Total L1 Elapsed Cycles                        13,037,426
    Average L2 Active Cycles                        83,294.81
    Total L2 Elapsed Cycles                        16,590,636
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,974.45
    Total SM Elapsed Cycles                        13,037,426
    Average SMSP Active Cycles                      17,879.24
    Total SMSP Elapsed Cycles                      52,149,704
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 16.05%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.67% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.97%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.70% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 16.05%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.67% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,749
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 86.61%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,485,342.02
    SM Frequency                        1,124,584,881.41
    Elapsed Cycles                                88,806
    Memory Throughput                               9.53
    DRAM Throughput                                 3.26
    Duration                                      78,592
    L1/TEX Cache Throughput                        47.53
    L2 Cache Throughput                             7.66
    SM Active Cycles                           17,966.30
    Compute (SM) Throughput                         2.56
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.78
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.22%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        216,885,993,485.34
    Mem Busy                                               6.90
    Max Bandwidth                                          9.53
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           44.82
    Mem Pipes Busy                                         2.24
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.934%                                                                                     
          Out of the 16831936.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 8.938%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.47%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 95.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          95.14
    Warp Cycles Per Executed Instruction                        95.51
    Avg. Active Threads Per Warp                                31.75
    Avg. Not Predicated Off Threads Per Warp                    29.71
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.84%                                                                                     
          On average, each warp of this workload spends 79.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.8% of the total average of 95.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.49
    Executed Instructions                                     110,996
    Avg. Issued Instructions Per Scheduler                     188.24
    Issued Instructions                                       111,436
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.23
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.77%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,323
    Total DRAM Elapsed Cycles                      20,086,784
    Average L1 Active Cycles                        17,966.30
    Total L1 Elapsed Cycles                        13,257,400
    Average L2 Active Cycles                        82,771.13
    Total L2 Elapsed Cycles                        16,696,344
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,966.30
    Total SM Elapsed Cycles                        13,257,400
    Average SMSP Active Cycles                      17,873.23
    Total SMSP Elapsed Cycles                      53,029,600
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.77%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.62% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.73%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.81% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.77%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.62% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,728
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 85.52%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,991,771,637.55
    SM Frequency                        1,127,061,852.40
    Elapsed Cycles                                89,035
    Memory Throughput                               9.61
    DRAM Throughput                                 3.26
    Duration                                      78,752
    L1/TEX Cache Throughput                        47.59
    L2 Cache Throughput                             7.64
    SM Active Cycles                           17,946.28
    Compute (SM) Throughput                         2.58
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.79
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.21%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        216,445,347,419.75
    Mem Busy                                               6.84
    Max Bandwidth                                          9.61
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           44.60
    Mem Pipes Busy                                         2.26
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.925%                                                                                     
          Out of the 16842176.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 9.011%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.39%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 94.8 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          95.15
    Warp Cycles Per Executed Instruction                        95.54
    Avg. Active Threads Per Warp                                31.77
    Avg. Not Predicated Off Threads Per Warp                    29.73
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.69%                                                                                     
          On average, each warp of this workload spends 79.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.7% of the total average of 95.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.54
    Executed Instructions                                     111,024
    Avg. Issued Instructions Per Scheduler                     188.30
    Issued Instructions                                       111,475
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.71%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,323
    Total DRAM Elapsed Cycles                      20,119,040
    Average L1 Active Cycles                        17,946.28
    Total L1 Elapsed Cycles                        13,149,544
    Average L2 Active Cycles                        83,635.59
    Total L2 Elapsed Cycles                        16,725,416
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,946.28
    Total SM Elapsed Cycles                        13,149,544
    Average SMSP Active Cycles                      17,854.50
    Total SMSP Elapsed Cycles                      52,598,176
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.9%                                                                                      
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.70% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.82%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.70% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.9%                                                                                      
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.70% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,722
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 86.26%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,991,794,975.69
    SM Frequency                        1,127,198,452.69
    Elapsed Cycles                                89,298
    Memory Throughput                               9.60
    DRAM Throughput                                 3.25
    Duration                                      78,976
    L1/TEX Cache Throughput                        47.67
    L2 Cache Throughput                             7.60
    SM Active Cycles                           17,915.15
    Compute (SM) Throughput                         2.58
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.81
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.19%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        215,831,442,463.53
    Mem Busy                                               6.90
    Max Bandwidth                                          9.60
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           43.78
    Mem Pipes Busy                                         2.25
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.926%                                                                                     
          Out of the 16831936.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 9.003%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.4%                                                                                      
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 95.2 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          94.79
    Warp Cycles Per Executed Instruction                        95.25
    Avg. Active Threads Per Warp                                31.76
    Avg. Not Predicated Off Threads Per Warp                    29.71
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.93%                                                                                     
          On average, each warp of this workload spends 79.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.9% of the total average of 94.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.51
    Executed Instructions                                     111,008
    Avg. Issued Instructions Per Scheduler                     188.41
    Issued Instructions                                       111,538
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.23
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.76%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,323
    Total DRAM Elapsed Cycles                      20,176,384
    Average L1 Active Cycles                        17,915.15
    Total L1 Elapsed Cycles                        13,160,922
    Average L2 Active Cycles                        82,594.11
    Total L2 Elapsed Cycles                        16,771,968
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,915.15
    Total SM Elapsed Cycles                        13,160,922
    Average SMSP Active Cycles                      17,934.85
    Total SMSP Elapsed Cycles                      52,643,688
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.85%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.65% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.87%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.70% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.85%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.65% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,764
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 84.95%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,992,355,204.54
    SM Frequency                        1,126,893,795.57
    Elapsed Cycles                                89,334
    Memory Throughput                               9.57
    DRAM Throughput                                 3.24
    Duration                                      79,008
    L1/TEX Cache Throughput                        47.72
    L2 Cache Throughput                             7.61
    SM Active Cycles                           17,897.24
    Compute (SM) Throughput                         2.57
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.83
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.17%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        215,747,266,099.64
    Mem Busy                                               6.82
    Max Bandwidth                                          9.57
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           43.75
    Mem Pipes Busy                                         2.25
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.922%                                                                                     
          Out of the 16831616.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 8.976%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.43%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 94.9 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          95.32
    Warp Cycles Per Executed Instruction                        95.68
    Avg. Active Threads Per Warp                                31.74
    Avg. Not Predicated Off Threads Per Warp                    29.70
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.28%                                                                                     
          On average, each warp of this workload spends 79.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.3% of the total average of 95.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.57
    Executed Instructions                                     111,044
    Avg. Issued Instructions Per Scheduler                     188.28
    Issued Instructions                                       111,463
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.27
    Achieved Active Warps Per SM                        4.01
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.57%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,323.12
    Total DRAM Elapsed Cycles                      20,187,392
    Average L1 Active Cycles                        17,897.24
    Total L1 Elapsed Cycles                        13,201,556
    Average L2 Active Cycles                        83,235.89
    Total L2 Elapsed Cycles                        16,782,364
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,897.24
    Total SM Elapsed Cycles                        13,201,556
    Average SMSP Active Cycles                      17,871.97
    Total SMSP Elapsed Cycles                      52,806,224
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.8%                                                                                      
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.76% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.76%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.66% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.8%                                                                                      
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.76% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,752
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 85.56%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,990,384,615.38
    SM Frequency                        1,126,616,270.24
    Elapsed Cycles                                89,372
    Memory Throughput                               9.65
    DRAM Throughput                                 3.24
    Duration                                      79,040
    L1/TEX Cache Throughput                        47.76
    L2 Cache Throughput                             7.62
    SM Active Cycles                           17,880.44
    Compute (SM) Throughput                         2.59
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.84
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.16%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        215,659,919,028.34
    Mem Busy                                               6.88
    Max Bandwidth                                          9.65
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           42.91
    Mem Pipes Busy                                         2.26
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.923%                                                                                     
          Out of the 16831488.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 9.046%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.35%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 95.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          94.93
    Warp Cycles Per Executed Instruction                        95.29
    Avg. Active Threads Per Warp                                31.76
    Avg. Not Predicated Off Threads Per Warp                    29.71
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.82%                                                                                     
          On average, each warp of this workload spends 79.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.8% of the total average of 94.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.61
    Executed Instructions                                     111,068
    Avg. Issued Instructions Per Scheduler                     188.32
    Issued Instructions                                       111,487
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.25
    Achieved Active Warps Per SM                        4.00
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.67%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,323.12
    Total DRAM Elapsed Cycles                      20,185,600
    Average L1 Active Cycles                        17,880.44
    Total L1 Elapsed Cycles                        13,098,892
    Average L2 Active Cycles                        83,348.82
    Total L2 Elapsed Cycles                        16,788,068
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,880.44
    Total SM Elapsed Cycles                        13,098,892
    Average SMSP Active Cycles                      17,896.32
    Total SMSP Elapsed Cycles                      52,395,568
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.89%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.68% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.92%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.71% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.89%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.68% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,746
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 85.64%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,367,346.94
    SM Frequency                        1,126,680,484.69
    Elapsed Cycles                                88,598
    Memory Throughput                               9.59
    DRAM Throughput                                 3.27
    Duration                                      78,400
    L1/TEX Cache Throughput                        47.61
    L2 Cache Throughput                             7.71
    SM Active Cycles                           17,938.31
    Compute (SM) Throughput                         2.58
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.79
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.21%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        217,420,408,163.27
    Mem Busy                                               6.73
    Max Bandwidth                                          9.59
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           44.76
    Mem Pipes Busy                                         2.25
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.947%                                                                                     
          Out of the 16839680.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 8.988%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.06
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.94
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.41%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 94.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          94.76
    Warp Cycles Per Executed Instruction                        95.14
    Avg. Active Threads Per Warp                                31.75
    Avg. Not Predicated Off Threads Per Warp                    29.71
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 84.05%                                                                                     
          On average, each warp of this workload spends 79.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 84.1% of the total average of 94.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.53
    Executed Instructions                                     111,020
    Avg. Issued Instructions Per Scheduler                     188.28
    Issued Instructions                                       111,463
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.22
    Achieved Active Warps Per SM                        3.98
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.84%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,323.12
    Total DRAM Elapsed Cycles                      20,037,120
    Average L1 Active Cycles                        17,938.31
    Total L1 Elapsed Cycles                        13,183,780
    Average L2 Active Cycles                        83,032.67
    Total L2 Elapsed Cycles                        16,653,932
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,938.31
    Total SM Elapsed Cycles                        13,183,780
    Average SMSP Active Cycles                      17,822.80
    Total SMSP Elapsed Cycles                      52,735,120
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.85%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.72% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.78%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.85% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.85%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.72% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,710
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 86%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,991,484,184.91
    SM Frequency                        1,126,851,746.25
    Elapsed Cycles                                89,233
    Memory Throughput                               9.68
    DRAM Throughput                                 3.25
    Duration                                      78,912
    L1/TEX Cache Throughput                        47.57
    L2 Cache Throughput                             7.67
    SM Active Cycles                           17,952.28
    Compute (SM) Throughput                         2.60
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.79
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.21%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        216,006,488,240.06
    Mem Busy                                               6.93
    Max Bandwidth                                          9.68
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           42.61
    Mem Pipes Busy                                         2.27
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.94%                                                                                      
          Out of the 16836000.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 9.074%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.32%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 95.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          94.90
    Warp Cycles Per Executed Instruction                        95.31
    Avg. Active Threads Per Warp                                31.73
    Avg. Not Predicated Off Threads Per Warp                    29.69
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.85%                                                                                     
          On average, each warp of this workload spends 79.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.9% of the total average of 94.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.64
    Executed Instructions                                     111,080
    Avg. Issued Instructions Per Scheduler                     188.45
    Issued Instructions                                       111,562
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.23
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.79%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,323
    Total DRAM Elapsed Cycles                      20,158,464
    Average L1 Active Cycles                        17,952.28
    Total L1 Elapsed Cycles                        13,059,064
    Average L2 Active Cycles                        83,757.93
    Total L2 Elapsed Cycles                        16,762,124
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,952.28
    Total SM Elapsed Cycles                        13,059,064
    Average SMSP Active Cycles                      17,899.38
    Total SMSP Elapsed Cycles                      52,236,256
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 16.02%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.73% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.97%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.75% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 16.02%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.73% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,728
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 86.2%                                                                                      
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,991,806,636.62
    SM Frequency                        1,126,550,658.03
    Elapsed Cycles                                88,260
    Memory Throughput                               9.59
    DRAM Throughput                                 3.28
    Duration                                      78,112
    L1/TEX Cache Throughput                        47.83
    L2 Cache Throughput                             7.70
    SM Active Cycles                           17,855.66
    Compute (SM) Throughput                         2.58
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.06
    Issued Ipc Active                        0.04
    SM Busy                                 12.85
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.15%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        218,222,040,147.48
    Mem Busy                                               6.91
    Max Bandwidth                                          9.59
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           42.94
    Mem Pipes Busy                                         2.25
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.949%                                                                                     
          Out of the 16832000.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 8.993%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.41%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 95.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          94.80
    Warp Cycles Per Executed Instruction                        95.28
    Avg. Active Threads Per Warp                                31.75
    Avg. Not Predicated Off Threads Per Warp                    29.71
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.97%                                                                                     
          On average, each warp of this workload spends 79.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 84.0% of the total average of 94.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.47
    Executed Instructions                                     110,984
    Avg. Issued Instructions Per Scheduler                     188.42
    Issued Instructions                                       111,547
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.25
    Achieved Active Warps Per SM                        4.00
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.65%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       8,323.12
    Total DRAM Elapsed Cycles                      19,955,712
    Average L1 Active Cycles                        17,855.66
    Total L1 Elapsed Cycles                        13,176,456
    Average L2 Active Cycles                        83,468.27
    Total L2 Elapsed Cycles                        16,587,324
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,855.66
    Total SM Elapsed Cycles                        13,176,456
    Average SMSP Active Cycles                      17,915.17
    Total SMSP Elapsed Cycles                      52,705,824
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.81%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.83% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.86%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.79% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.81%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.83% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,772
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 86.8%                                                                                      
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 32, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,442,622.95
    SM Frequency                        1,126,452,036.37
    Elapsed Cycles                                88,242
    Memory Throughput                               9.55
    DRAM Throughput                                 3.28
    Duration                                      78,080
    L1/TEX Cache Throughput                        47.70
    L2 Cache Throughput                             7.67
    SM Active Cycles                           17,902.74
    Compute (SM) Throughput                         2.57
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         1.05
    Issued Ipc Active                        0.04
    SM Busy                                 12.82
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 87.18%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        218,308,196,721.31
    Mem Busy                                               6.85
    Max Bandwidth                                          9.55
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           44.06
    Mem Pipes Busy                                         2.24
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 1.947%                                                                                     
          Out of the 16831552.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 8.956%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             1.05
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     98.95
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 90.45%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 95.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                          94.92
    Warp Cycles Per Executed Instruction                        95.31
    Avg. Active Threads Per Warp                                31.77
    Avg. Not Predicated Off Threads Per Warp                    29.72
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.77%                                                                                     
          On average, each warp of this workload spends 79.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 83.8% of the total average of 94.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   187.49
    Executed Instructions                                     110,996
    Avg. Issued Instructions Per Scheduler                     188.26
    Issued Instructions                                       111,448
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 32
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                4,096
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.02
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 78.38%                                                                                     
          The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.72%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          8,323
    Total DRAM Elapsed Cycles                      19,955,712
    Average L1 Active Cycles                        17,902.74
    Total L1 Elapsed Cycles                        13,230,698
    Average L2 Active Cycles                        83,070.40
    Total L2 Elapsed Cycles                        16,586,036
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        17,902.74
    Total SM Elapsed Cycles                        13,230,698
    Average SMSP Active Cycles                      17,879.36
    Total SMSP Elapsed Cycles                      52,922,792
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 15.78%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.78% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.75%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.74% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 15.78%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.78% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.05
    Branch Instructions                          5,740
    Branch Efficiency                            99.57
    Avg. Divergent Branches                       0.05
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 86.4%                                                                                      
          This kernel has uncoalesced global accesses resulting in a total of 491520 excessive sectors (94% of the      
          total 524288 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,992,237,687.37
    SM Frequency                        1,138,648,872.46
    Elapsed Cycles                                68,245
    Memory Throughput                              18.56
    DRAM Throughput                                 2.12
    Duration                                      59,776
    L1/TEX Cache Throughput                        56.99
    L2 Cache Throughput                            15.31
    SM Active Cycles                           22,524.20
    Compute (SM) Throughput                         2.80
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.94
    Issued Ipc Active                        0.04
    SM Busy                                  5.09
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.91%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        140,847,965,738.76
    Mem Busy                                              12.31
    Max Bandwidth                                         18.56
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           52.56
    Mem Pipes Busy                                         2.80
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.8988%                                                                                    
          Out of the 29408416.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 17.4%                                                                                      
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.94
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.06
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 81.44%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 106.2 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         105.83
    Warp Cycles Per Executed Instruction                       105.88
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.83
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.76%                                                                                     
          On average, each warp of this workload spends 79.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.8% of the total average of 105.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.21
    Achieved Active Warps Per SM                        3.98
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.86%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.25%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          4,111
    Total DRAM Elapsed Cycles                      15,272,960
    Average L1 Active Cycles                        22,524.20
    Total L1 Elapsed Cycles                        10,234,396
    Average L2 Active Cycles                        61,604.66
    Total L2 Elapsed Cycles                        12,695,356
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        22,524.20
    Total SM Elapsed Cycles                        10,234,396
    Average SMSP Active Cycles                      22,455.32
    Total SMSP Elapsed Cycles                      40,937,584
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 21.14%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 64.92% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.06%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.85% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.14%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.92% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.71%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,989,373,297.00
    SM Frequency                        1,124,542,319.48
    Elapsed Cycles                                66,812
    Memory Throughput                              19.11
    DRAM Throughput                                 2.16
    Duration                                      58,720
    L1/TEX Cache Throughput                        58.67
    L2 Cache Throughput                            15.60
    SM Active Cycles                           21,881.02
    Compute (SM) Throughput                         2.84
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.97
    Issued Ipc Active                        0.04
    SM Busy                                  5.24
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.76%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        143,385,286,103.54
    Mem Busy                                              12.82
    Max Bandwidth                                         19.11
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           53.38
    Mem Pipes Busy                                         2.84
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.9121%                                                                                    
          Out of the 29408480.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 17.92%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.97
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.03
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.89%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 103.2 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         103.06
    Warp Cycles Per Executed Instruction                       103.11
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.48%                                                                                     
          On average, each warp of this workload spends 76.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.5% of the total average of 103.1 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.43
    Executed Instructions                                     125,164
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.23
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.78%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.89%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       4,111.12
    Total DRAM Elapsed Cycles                      14,992,384
    Average L1 Active Cycles                        21,881.02
    Total L1 Elapsed Cycles                         9,940,986
    Average L2 Active Cycles                        60,050.23
    Total L2 Elapsed Cycles                        12,467,472
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        21,881.02
    Total SM Elapsed Cycles                         9,940,986
    Average SMSP Active Cycles                      21,822.07
    Total SMSP Elapsed Cycles                      39,763,944
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 20.9%                                                                                      
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 64.15% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.04%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.76% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 20.9%                                                                                      
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.15% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.09%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,990,239,633.82
    SM Frequency                        1,120,321,755.52
    Elapsed Cycles                                67,513
    Memory Throughput                              19.25
    DRAM Throughput                                 2.13
    Duration                                      59,424
    L1/TEX Cache Throughput                        57.74
    L2 Cache Throughput                            15.40
    SM Active Cycles                           22,230.63
    Compute (SM) Throughput                         2.83
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.95
    Issued Ipc Active                        0.04
    SM Busy                                  5.16
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.84%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        141,686,591,276.25
    Mem Busy                                              12.91
    Max Bandwidth                                         19.25
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           53.96
    Mem Pipes Busy                                         2.83
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.9006%                                                                                    
          Out of the 29408064.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 18.05%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.96
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.04
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.75%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 104.2 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         104.06
    Warp Cycles Per Executed Instruction                       104.11
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.46%                                                                                     
          On average, each warp of this workload spends 77.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.5% of the total average of 104.1 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.19
    Achieved Active Warps Per SM                        3.96
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.99%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.75%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       4,111.12
    Total DRAM Elapsed Cycles                      15,175,424
    Average L1 Active Cycles                        22,230.63
    Total L1 Elapsed Cycles                         9,867,208
    Average L2 Active Cycles                        59,655.27
    Total L2 Elapsed Cycles                        12,617,432
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        22,230.63
    Total SM Elapsed Cycles                         9,867,208
    Average SMSP Active Cycles                      22,048.95
    Total SMSP Elapsed Cycles                      39,468,832
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 21.38%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 64.13% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.26%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.28% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.38%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.13% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 81.56%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,989,434,687.16
    SM Frequency                        1,124,097,403.27
    Elapsed Cycles                                66,342
    Memory Throughput                              19.20
    DRAM Throughput                                 2.17
    Duration                                      58,304
    L1/TEX Cache Throughput                        58.55
    L2 Cache Throughput                            15.70
    SM Active Cycles                           21,923.07
    Compute (SM) Throughput                         2.85
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.96
    Issued Ipc Active                        0.04
    SM Busy                                  5.23
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.77%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        144,403,951,701.43
    Mem Busy                                              12.89
    Max Bandwidth                                         19.20
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           52.90
    Mem Pipes Busy                                         2.85
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.9201%                                                                                    
          Out of the 29404512.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 18%                                                                                        
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.97
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.03
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.8%                                                                                      
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 103.3 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         103.15
    Warp Cycles Per Executed Instruction                       103.19
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.41%                                                                                     
          On average, each warp of this workload spends 76.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.4% of the total average of 103.1 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.22
    Achieved Active Warps Per SM                        3.98
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.82%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.8%                                                                                      
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          4,111
    Total DRAM Elapsed Cycles                      14,886,400
    Average L1 Active Cycles                        21,923.07
    Total L1 Elapsed Cycles                         9,895,106
    Average L2 Active Cycles                        60,679.66
    Total L2 Elapsed Cycles                        12,380,072
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        21,923.07
    Total SM Elapsed Cycles                         9,895,106
    Average SMSP Active Cycles                      21,859.05
    Total SMSP Elapsed Cycles                      39,580,424
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 21.03%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 64.14% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.01%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.28% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.03%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.14% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 84.55%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,992,058,195.41
    SM Frequency                        1,123,231,446.88
    Elapsed Cycles                                68,258
    Memory Throughput                              18.91
    DRAM Throughput                                 2.11
    Duration                                      59,936
    L1/TEX Cache Throughput                        58.01
    L2 Cache Throughput                            15.29
    SM Active Cycles                           22,126.77
    Compute (SM) Throughput                         2.78
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.96
    Issued Ipc Active                        0.04
    SM Busy                                  5.19
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.81%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        140,471,970,101.44
    Mem Busy                                              12.69
    Max Bandwidth                                         18.91
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           53.85
    Mem Pipes Busy                                         2.78
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.8871%                                                                                    
          Out of the 29404480.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 17.73%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.96
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.04
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 81.09%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 104.5 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         104.45
    Warp Cycles Per Executed Instruction                       104.50
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.25%                                                                                     
          On average, each warp of this workload spends 77.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.3% of the total average of 104.5 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        4.00
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.71%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 81.09%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          4,111
    Total DRAM Elapsed Cycles                      15,313,152
    Average L1 Active Cycles                        22,126.77
    Total L1 Elapsed Cycles                        10,045,372
    Average L2 Active Cycles                        60,388.84
    Total L2 Elapsed Cycles                        12,723,508
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        22,126.77
    Total SM Elapsed Cycles                        10,045,372
    Average SMSP Active Cycles                      22,095.67
    Total SMSP Elapsed Cycles                      40,181,488
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 20.98%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 64.35% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 20.95%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.34% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 20.98%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.35% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 81.87%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,992,213,340.68
    SM Frequency                        1,120,880,564.36
    Elapsed Cycles                                65,817
    Memory Throughput                              19.29
    DRAM Throughput                                 2.18
    Duration                                      58,048
    L1/TEX Cache Throughput                        58.73
    L2 Cache Throughput                            15.78
    SM Active Cycles                           21,856.14
    Compute (SM) Throughput                         2.87
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.97
    Issued Ipc Active                        0.04
    SM Busy                                  5.25
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.75%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        145,040,793,825.80
    Mem Busy                                              12.95
    Max Bandwidth                                         19.29
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           53.28
    Mem Pipes Busy                                         2.87
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.9162%                                                                                    
          Out of the 29408992.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 18.08%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.97
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.03
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.71%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 102.8 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         102.96
    Warp Cycles Per Executed Instruction                       103.01
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.84%                                                                                     
          On average, each warp of this workload spends 77.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.8% of the total average of 103.0 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.23
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.78%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.71%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          4,111
    Total DRAM Elapsed Cycles                      14,831,360
    Average L1 Active Cycles                        21,856.14
    Total L1 Elapsed Cycles                         9,850,608
    Average L2 Active Cycles                        59,957.80
    Total L2 Elapsed Cycles                        12,328,092
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        21,856.14
    Total SM Elapsed Cycles                         9,850,608
    Average SMSP Active Cycles                      21,752.76
    Total SMSP Elapsed Cycles                      39,402,432
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 20.99%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.92% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.09%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.53% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 20.99%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.92% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.9%                                                                                      
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,991,554,959.79
    SM Frequency                        1,120,568,029.49
    Elapsed Cycles                                67,949
    Memory Throughput                              19.37
    DRAM Throughput                                 2.12
    Duration                                      59,680
    L1/TEX Cache Throughput                        57.65
    L2 Cache Throughput                            15.34
    SM Active Cycles                           22,265.07
    Compute (SM) Throughput                         2.85
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.95
    Issued Ipc Active                        0.04
    SM Busy                                  5.15
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.85%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        141,074,530,831.10
    Mem Busy                                              12.99
    Max Bandwidth                                         19.37
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           53.93
    Mem Pipes Busy                                         2.85
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.8926%                                                                                    
          Out of the 29408192.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 18.16%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.96
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.04
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.63%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 104.2 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         104.65
    Warp Cycles Per Executed Instruction                       104.70
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.05%                                                                                     
          On average, each warp of this workload spends 77.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.1% of the total average of 104.7 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.22
    Achieved Active Warps Per SM                        3.98
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.85%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.63%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          4,111
    Total DRAM Elapsed Cycles                      15,245,824
    Average L1 Active Cycles                        22,265.07
    Total L1 Elapsed Cycles                         9,810,390
    Average L2 Active Cycles                        59,688.26
    Total L2 Elapsed Cycles                        12,676,680
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        22,265.07
    Total SM Elapsed Cycles                         9,810,390
    Average SMSP Active Cycles                      22,039.71
    Total SMSP Elapsed Cycles                      39,241,560
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 21.56%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 64.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.38%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.30% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.56%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.19% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 81.22%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,388,429.75
    SM Frequency                        1,125,335,743.80
    Elapsed Cycles                                66,115
    Memory Throughput                              19.03
    DRAM Throughput                                 2.18
    Duration                                      58,080
    L1/TEX Cache Throughput                        58.73
    L2 Cache Throughput                            15.79
    SM Active Cycles                           21,855.77
    Compute (SM) Throughput                         2.83
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.97
    Issued Ipc Active                        0.04
    SM Busy                                  5.25
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.75%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        144,965,289,256.20
    Mem Busy                                              12.77
    Max Bandwidth                                         19.03
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           52.77
    Mem Pipes Busy                                         2.83
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.919%                                                                                     
          Out of the 29406336.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 17.84%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.97
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.03
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.97%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 103.0 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         103.06
    Warp Cycles Per Executed Instruction                       103.10
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.58%                                                                                     
          On average, each warp of this workload spends 76.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.6% of the total average of 103.1 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.74%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.97%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       4,111.12
    Total DRAM Elapsed Cycles                      14,843,904
    Average L1 Active Cycles                        21,855.77
    Total L1 Elapsed Cycles                         9,983,190
    Average L2 Active Cycles                        59,539.09
    Total L2 Elapsed Cycles                        12,334,440
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        21,855.77
    Total SM Elapsed Cycles                         9,983,190
    Average SMSP Active Cycles                      21,784.92
    Total SMSP Elapsed Cycles                      39,932,760
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 20.69%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.85% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 20.85%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.55% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 20.69%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.85% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.27%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,989,299,090.42
    SM Frequency                        1,122,168,021.00
    Elapsed Cycles                                68,084
    Memory Throughput                              19.21
    DRAM Throughput                                 2.12
    Duration                                      59,808
    L1/TEX Cache Throughput                        57.96
    L2 Cache Throughput                            15.33
    SM Active Cycles                           22,148.30
    Compute (SM) Throughput                         2.82
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.95
    Issued Ipc Active                        0.04
    SM Busy                                  5.18
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.82%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        140,776,886,035.31
    Mem Busy                                              12.90
    Max Bandwidth                                         19.21
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           53.15
    Mem Pipes Busy                                         2.82
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.8883%                                                                                    
          Out of the 29408160.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 18.01%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.96
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.04
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.79%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 104.1 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         104.57
    Warp Cycles Per Executed Instruction                       104.62
    Avg. Active Threads Per Warp                                31.63
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.15%                                                                                     
          On average, each warp of this workload spends 77.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.1% of the total average of 104.6 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        4.00
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.7%                                                                                      
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.79%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       4,111.12
    Total DRAM Elapsed Cycles                      15,269,888
    Average L1 Active Cycles                        22,148.30
    Total L1 Elapsed Cycles                         9,890,422
    Average L2 Active Cycles                        60,170.68
    Total L2 Elapsed Cycles                        12,692,044
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        22,148.30
    Total SM Elapsed Cycles                         9,890,422
    Average SMSP Active Cycles                      22,015.42
    Total SMSP Elapsed Cycles                      39,561,688
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 21.31%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 64.29% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.21%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.39% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.31%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.29% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 81.78%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,990,019,273.13
    SM Frequency                        1,125,118,306.03
    Elapsed Cycles                                66,139
    Memory Throughput                              19.04
    DRAM Throughput                                 2.18
    Duration                                      58,112
    L1/TEX Cache Throughput                        58.71
    L2 Cache Throughput                            15.76
    SM Active Cycles                           21,865.16
    Compute (SM) Throughput                         2.83
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.97
    Issued Ipc Active                        0.04
    SM Busy                                  5.25
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.75%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        144,881,057,268.72
    Mem Busy                                              12.78
    Max Bandwidth                                         19.04
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           52.81
    Mem Pipes Busy                                         2.83
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.9218%                                                                                    
          Out of the 29408480.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 17.85%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.97
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.03
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.96%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 103.1 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         103.17
    Warp Cycles Per Executed Instruction                       103.22
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.56%                                                                                     
          On average, each warp of this workload spends 76.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.6% of the total average of 103.2 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.72%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.96%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          4,111
    Total DRAM Elapsed Cycles                      14,839,552
    Average L1 Active Cycles                        21,865.16
    Total L1 Elapsed Cycles                         9,979,374
    Average L2 Active Cycles                        60,349.46
    Total L2 Elapsed Cycles                        12,337,844
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        21,865.16
    Total SM Elapsed Cycles                         9,979,374
    Average SMSP Active Cycles                      21,815.72
    Total SMSP Elapsed Cycles                      39,917,496
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 20.72%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.88% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 20.96%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.80% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 20.72%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.88% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 84.38%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,081,676.52
    SM Frequency                        1,123,778,378.56
    Elapsed Cycles                                67,793
    Memory Throughput                              19.17
    DRAM Throughput                                 2.13
    Duration                                      59,552
    L1/TEX Cache Throughput                        57.96
    L2 Cache Throughput                            15.40
    SM Active Cycles                           22,147.66
    Compute (SM) Throughput                         2.82
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.96
    Issued Ipc Active                        0.04
    SM Busy                                  5.18
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.82%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        141,382,052,659.86
    Mem Busy                                              12.86
    Max Bandwidth                                         19.17
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           53.43
    Mem Pipes Busy                                         2.82
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.8986%                                                                                    
          Out of the 29406528.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 17.97%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.96
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.04
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.83%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 104.4 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         104.11
    Warp Cycles Per Executed Instruction                       104.16
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.76%                                                                                     
          On average, each warp of this workload spends 77.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.8% of the total average of 104.1 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.43
    Executed Instructions                                     125,164
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.22
    Achieved Active Warps Per SM                        3.98
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.85%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.83%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       4,111.12
    Total DRAM Elapsed Cycles                      15,218,944
    Average L1 Active Cycles                        22,147.66
    Total L1 Elapsed Cycles                         9,910,610
    Average L2 Active Cycles                        58,861.73
    Total L2 Elapsed Cycles                        12,643,652
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        22,147.66
    Total SM Elapsed Cycles                         9,910,610
    Average SMSP Active Cycles                      22,092.30
    Total SMSP Elapsed Cycles                      39,642,440
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 21.3%                                                                                      
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 64.39% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.29%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.52% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.3%                                                                                      
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.39% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.31%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,417,443.77
    SM Frequency                        1,123,026,518.79
    Elapsed Cycles                                66,379
    Memory Throughput                              19.19
    DRAM Throughput                                 2.17
    Duration                                      58,336
    L1/TEX Cache Throughput                        58.58
    L2 Cache Throughput                            15.70
    SM Active Cycles                           21,911.32
    Compute (SM) Throughput                         2.85
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.97
    Issued Ipc Active                        0.04
    SM Busy                                  5.24
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.76%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        144,324,739,440.48
    Mem Busy                                              12.90
    Max Bandwidth                                         19.19
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           53.27
    Mem Pipes Busy                                         2.85
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.9115%                                                                                    
          Out of the 29408640.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 17.99%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.97
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.03
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.81%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 103.1 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         103.08
    Warp Cycles Per Executed Instruction                       103.13
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.62%                                                                                     
          On average, each warp of this workload spends 76.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.6% of the total average of 103.1 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.22
    Achieved Active Warps Per SM                        3.98
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.82%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.81%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          4,111
    Total DRAM Elapsed Cycles                      14,909,440
    Average L1 Active Cycles                        21,911.32
    Total L1 Elapsed Cycles                         9,900,238
    Average L2 Active Cycles                        60,094.38
    Total L2 Elapsed Cycles                        12,389,916
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        21,911.32
    Total SM Elapsed Cycles                         9,900,238
    Average SMSP Active Cycles                      21,804.25
    Total SMSP Elapsed Cycles                      39,600,952
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 20.98%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 64.04% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.11%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.76% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 20.98%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.04% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.67%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,989,145,091.69
    SM Frequency                        1,122,981,560.14
    Elapsed Cycles                                67,519
    Memory Throughput                              19.18
    DRAM Throughput                                 2.14
    Duration                                      59,328
    L1/TEX Cache Throughput                        57.74
    L2 Cache Throughput                            15.44
    SM Active Cycles                           22,232.95
    Compute (SM) Throughput                         2.82
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.95
    Issued Ipc Active                        0.04
    SM Busy                                  5.16
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.84%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        141,911,542,610.57
    Mem Busy                                              12.86
    Max Bandwidth                                         19.18
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           53.32
    Mem Pipes Busy                                         2.82
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.8978%                                                                                    
          Out of the 29408384.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 17.98%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.96
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.04
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.82%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 104.4 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         104.26
    Warp Cycles Per Executed Instruction                       104.31
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.31%                                                                                     
          On average, each warp of this workload spends 77.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.3% of the total average of 104.3 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.43
    Executed Instructions                                     125,164
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.20
    Achieved Active Warps Per SM                        3.97
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.93%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.82%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          4,111
    Total DRAM Elapsed Cycles                      15,146,752
    Average L1 Active Cycles                        22,232.95
    Total L1 Elapsed Cycles                         9,906,142
    Average L2 Active Cycles                        60,337.91
    Total L2 Elapsed Cycles                        12,593,420
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        22,232.95
    Total SM Elapsed Cycles                         9,906,142
    Average SMSP Active Cycles                      22,078.40
    Total SMSP Elapsed Cycles                      39,624,568
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 21.33%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 64.20% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.23%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.36% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.33%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.20% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 82.65%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,989,991,717.28
    SM Frequency                        1,122,704,997.24
    Elapsed Cycles                                65,922
    Memory Throughput                              19.13
    DRAM Throughput                                 2.19
    Duration                                      57,952
    L1/TEX Cache Throughput                        58.31
    L2 Cache Throughput                            15.82
    SM Active Cycles                           22,016.01
    Compute (SM) Throughput                         2.84
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.96
    Issued Ipc Active                        0.04
    SM Busy                                  5.21
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.79%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        145,285,477,636.66
    Mem Busy                                              12.85
    Max Bandwidth                                         19.13
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           53.70
    Mem Pipes Busy                                         2.84
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.9198%                                                                                    
          Out of the 29407776.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 17.93%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.97
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.03
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.87%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 103.2 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         103.04
    Warp Cycles Per Executed Instruction                       103.08
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 75.2%                                                                                      
          On average, each warp of this workload spends 77.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 75.2% of the total average of 103.0 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.19
    Achieved Active Warps Per SM                        3.96
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.99%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.87%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       4,111.12
    Total DRAM Elapsed Cycles                      14,798,592
    Average L1 Active Cycles                        22,016.01
    Total L1 Elapsed Cycles                         9,930,760
    Average L2 Active Cycles                        60,624.42
    Total L2 Elapsed Cycles                        12,299,388
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        22,016.01
    Total SM Elapsed Cycles                         9,930,760
    Average SMSP Active Cycles                      21,823.11
    Total SMSP Elapsed Cycles                      39,723,040
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 20.9%                                                                                      
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.71% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 20.99%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.54% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 20.9%                                                                                      
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.71% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 85.03%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,096,123.21
    SM Frequency                        1,122,973,230.88
    Elapsed Cycles                                68,609
    Memory Throughput                              19.37
    DRAM Throughput                                 2.10
    Duration                                      60,256
    L1/TEX Cache Throughput                        57.99
    L2 Cache Throughput                            15.20
    SM Active Cycles                           22,136.22
    Compute (SM) Throughput                         2.85
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.96
    Issued Ipc Active                        0.04
    SM Busy                                  5.18
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.82%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        139,725,969,198.09
    Mem Busy                                              13.01
    Max Bandwidth                                         19.37
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           52.61
    Mem Pipes Busy                                         2.85
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.8868%                                                                                    
          Out of the 29404512.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 18.16%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.96
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.04
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.63%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 104.4 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         104.19
    Warp Cycles Per Executed Instruction                       104.24
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.5%                                                                                      
          On average, each warp of this workload spends 77.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.5% of the total average of 104.2 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.22
    Achieved Active Warps Per SM                        3.98
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.8%                                                                                      
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.63%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          4,111
    Total DRAM Elapsed Cycles                      15,398,912
    Average L1 Active Cycles                        22,136.22
    Total L1 Elapsed Cycles                         9,807,548
    Average L2 Active Cycles                        60,376.63
    Total L2 Elapsed Cycles                        12,797,568
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        22,136.22
    Total SM Elapsed Cycles                         9,807,548
    Average SMSP Active Cycles                      22,091.78
    Total SMSP Elapsed Cycles                      39,230,192
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 21.52%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 64.43% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.44%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.31% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.52%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.43% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 81.38%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,990,707,599.12
    SM Frequency                        1,120,424,783.18
    Elapsed Cycles                                66,111
    Memory Throughput                              19.14
    DRAM Throughput                                 2.18
    Duration                                      58,112
    L1/TEX Cache Throughput                        58.68
    L2 Cache Throughput                            15.78
    SM Active Cycles                           21,874.58
    Compute (SM) Throughput                         2.84
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.97
    Issued Ipc Active                        0.04
    SM Busy                                  5.25
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.75%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        144,885,462,555.07
    Mem Busy                                              12.84
    Max Bandwidth                                         19.14
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           52.85
    Mem Pipes Busy                                         2.84
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.9233%                                                                                    
          Out of the 29408608.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 17.94%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.97
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.03
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.86%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 103.1 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         103.05
    Warp Cycles Per Executed Instruction                       103.09
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.6%                                                                                      
          On average, each warp of this workload spends 76.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.6% of the total average of 103.0 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.23
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.78%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.86%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       4,111.12
    Total DRAM Elapsed Cycles                      14,842,112
    Average L1 Active Cycles                        21,874.58
    Total L1 Elapsed Cycles                         9,928,070
    Average L2 Active Cycles                        59,659.47
    Total L2 Elapsed Cycles                        12,339,316
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        21,874.58
    Total SM Elapsed Cycles                         9,928,070
    Average SMSP Active Cycles                      21,801.90
    Total SMSP Elapsed Cycles                      39,712,280
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 20.85%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.94% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 20.96%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.48% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 20.85%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.94% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.4%                                                                                      
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,579,454.25
    SM Frequency                        1,125,823,468.43
    Elapsed Cycles                                68,117
    Memory Throughput                              19.34
    DRAM Throughput                                 2.12
    Duration                                      59,808
    L1/TEX Cache Throughput                        57.98
    L2 Cache Throughput                            15.31
    SM Active Cycles                           22,138.66
    Compute (SM) Throughput                         2.84
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.96
    Issued Ipc Active                        0.04
    SM Busy                                  5.18
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.82%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        140,772,605,671.48
    Mem Busy                                              12.97
    Max Bandwidth                                         19.34
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           52.74
    Mem Pipes Busy                                         2.84
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.8928%                                                                                    
          Out of the 29406464.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 18.13%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.96
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.04
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.66%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 104.1 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         104.18
    Warp Cycles Per Executed Instruction                       104.23
    Avg. Active Threads Per Warp                                31.63
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.27%                                                                                     
          On average, each warp of this workload spends 77.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.3% of the total average of 104.2 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.22
    Achieved Active Warps Per SM                        3.98
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.81%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.66%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          4,111
    Total DRAM Elapsed Cycles                      15,286,272
    Average L1 Active Cycles                        22,138.66
    Total L1 Elapsed Cycles                         9,822,180
    Average L2 Active Cycles                        60,499.46
    Total L2 Elapsed Cycles                        12,700,784
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        22,138.66
    Total SM Elapsed Cycles                         9,822,180
    Average SMSP Active Cycles                      22,015.72
    Total SMSP Elapsed Cycles                      39,288,720
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 21.46%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 64.34% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.34%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.32% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.46%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.34% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 82.17%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,993,295,019.16
    SM Frequency                        1,121,867,730.57
    Elapsed Cycles                                66,506
    Memory Throughput                              19.07
    DRAM Throughput                                 2.17
    Duration                                      58,464
    L1/TEX Cache Throughput                        58.65
    L2 Cache Throughput                            15.63
    SM Active Cycles                           21,885.03
    Compute (SM) Throughput                         2.83
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.97
    Issued Ipc Active                        0.04
    SM Busy                                  5.24
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.76%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        144,013,136,289.00
    Mem Busy                                              12.80
    Max Bandwidth                                         19.07
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           52.97
    Mem Pipes Busy                                         2.83
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.9151%                                                                                    
          Out of the 29383328.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 17.88%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.97
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.03
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.93%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 103.0 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         103.19
    Warp Cycles Per Executed Instruction                       103.23
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.51%                                                                                     
          On average, each warp of this workload spends 76.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.5% of the total average of 103.2 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.23
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.75%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.93%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       4,111.12
    Total DRAM Elapsed Cycles                      14,941,696
    Average L1 Active Cycles                        21,885.03
    Total L1 Elapsed Cycles                         9,962,704
    Average L2 Active Cycles                        60,416.91
    Total L2 Elapsed Cycles                        12,416,872
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        21,885.03
    Total SM Elapsed Cycles                         9,962,704
    Average SMSP Active Cycles                      21,790.62
    Total SMSP Elapsed Cycles                      39,850,816
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 20.81%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 64.02% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 20.9%                                                                                      
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.58% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 20.81%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.02% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.93%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,988,024,757.80
    SM Frequency                        1,121,198,869.75
    Elapsed Cycles                                67,636
    Memory Throughput                              19.12
    DRAM Throughput                                 2.13
    Duration                                      59,456
    L1/TEX Cache Throughput                        57.79
    L2 Cache Throughput                            15.41
    SM Active Cycles                           22,211.34
    Compute (SM) Throughput                         2.81
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.95
    Issued Ipc Active                        0.04
    SM Busy                                  5.17
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.83%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        141,610,333,692.14
    Mem Busy                                              12.83
    Max Bandwidth                                         19.12
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           53.27
    Mem Pipes Busy                                         2.81
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.9005%                                                                                    
          Out of the 29407104.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 17.93%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.96
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.04
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.88%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 104.0 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         104.15
    Warp Cycles Per Executed Instruction                       104.19
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.42%                                                                                     
          On average, each warp of this workload spends 77.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.4% of the total average of 104.1 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.20
    Achieved Active Warps Per SM                        3.97
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.93%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.88%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       4,111.12
    Total DRAM Elapsed Cycles                      15,175,168
    Average L1 Active Cycles                        22,211.34
    Total L1 Elapsed Cycles                         9,934,574
    Average L2 Active Cycles                        59,986.10
    Total L2 Elapsed Cycles                        12,618,260
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        22,211.34
    Total SM Elapsed Cycles                         9,934,574
    Average SMSP Active Cycles                      22,007.03
    Total SMSP Elapsed Cycles                      39,738,296
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 21.34%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 64.48% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.17%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.56% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.34%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.48% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 82%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,990,778,688.52
    SM Frequency                        1,123,697,916.67
    Elapsed Cycles                                66,665
    Memory Throughput                              19.25
    DRAM Throughput                                 2.16
    Duration                                      58,560
    L1/TEX Cache Throughput                        58.63
    L2 Cache Throughput                            15.61
    SM Active Cycles                           21,894.05
    Compute (SM) Throughput                         2.86
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.97
    Issued Ipc Active                        0.04
    SM Busy                                  5.24
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.76%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        143,772,677,595.63
    Mem Busy                                              12.91
    Max Bandwidth                                         19.25
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           52.67
    Mem Pipes Busy                                         2.86
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.9155%                                                                                    
          Out of the 29408064.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 18.05%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.97
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.03
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.75%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 103.0 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         103.07
    Warp Cycles Per Executed Instruction                       103.11
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.83%                                                                                     
          On average, each warp of this workload spends 77.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.8% of the total average of 103.1 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.23
    Achieved Active Warps Per SM                        3.98
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.8%                                                                                      
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.75%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          4,111
    Total DRAM Elapsed Cycles                      14,956,800
    Average L1 Active Cycles                        21,894.05
    Total L1 Elapsed Cycles                         9,870,038
    Average L2 Active Cycles                        60,235.57
    Total L2 Elapsed Cycles                        12,433,892
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        21,894.05
    Total SM Elapsed Cycles                         9,870,038
    Average SMSP Active Cycles                      21,784.14
    Total SMSP Elapsed Cycles                      39,480,152
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 20.95%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.82% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.15%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.76% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 20.95%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.82% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.57%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,990,810,810.81
    SM Frequency                        1,124,136,402.03
    Elapsed Cycles                                67,412
    Memory Throughput                              19.13
    DRAM Throughput                                 2.14
    Duration                                      59,200
    L1/TEX Cache Throughput                        57.99
    L2 Cache Throughput                            15.48
    SM Active Cycles                           22,137.03
    Compute (SM) Throughput                         2.81
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.96
    Issued Ipc Active                        0.04
    SM Busy                                  5.18
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.82%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        142,222,702,702.70
    Mem Busy                                              12.84
    Max Bandwidth                                         19.13
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           53.07
    Mem Pipes Busy                                         2.81
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.9015%                                                                                    
          Out of the 29405056.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 17.94%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.96
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.04
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.87%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 104.6 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         104.33
    Warp Cycles Per Executed Instruction                       104.37
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.85%                                                                                     
          On average, each warp of this workload spends 78.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.9% of the total average of 104.3 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.23
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.76%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.87%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                       4,111.12
    Total DRAM Elapsed Cycles                      15,120,384
    Average L1 Active Cycles                        22,137.03
    Total L1 Elapsed Cycles                         9,930,714
    Average L2 Active Cycles                        58,883.92
    Total L2 Elapsed Cycles                        12,564,992
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        22,137.03
    Total SM Elapsed Cycles                         9,930,714
    Average SMSP Active Cycles                      22,116.04
    Total SMSP Elapsed Cycles                      39,722,856
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 21.35%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 64.72% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.25%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.47% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.35%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.72% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.84%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,991,086,121.78
    SM Frequency                        1,124,995,714.48
    Elapsed Cycles                                66,387
    Memory Throughput                              19.33
    DRAM Throughput                                 2.17
    Duration                                      58,336
    L1/TEX Cache Throughput                        58.70
    L2 Cache Throughput                            15.69
    SM Active Cycles                           21,866.64
    Compute (SM) Throughput                         2.87
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.97
    Issued Ipc Active                        0.04
    SM Busy                                  5.25
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.75%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        144,324,739,440.48
    Mem Busy                                              12.97
    Max Bandwidth                                         19.33
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           53.31
    Mem Pipes Busy                                         2.87
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.9159%                                                                                    
          Out of the 29405088.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 18.12%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.97
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.03
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.67%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 103.1 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         102.91
    Warp Cycles Per Executed Instruction                       102.96
    Avg. Active Threads Per Warp                                31.63
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.61%                                                                                     
          On average, each warp of this workload spends 76.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.6% of the total average of 102.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.22
    Achieved Active Warps Per SM                        3.98
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.81%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.67%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          4,111
    Total DRAM Elapsed Cycles                      14,900,736
    Average L1 Active Cycles                        21,866.64
    Total L1 Elapsed Cycles                         9,829,322
    Average L2 Active Cycles                        60,759.91
    Total L2 Elapsed Cycles                        12,385,500
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        21,866.64
    Total SM Elapsed Cycles                         9,829,322
    Average SMSP Active Cycles                      21,817.47
    Total SMSP Elapsed Cycles                      39,317,288
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 21%                                                                                        
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.79% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.25%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.68% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21%                                                                                        
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.79% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 84.62%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,991,471,215.35
    SM Frequency                        1,122,293,110.34
    Elapsed Cycles                                68,314
    Memory Throughput                              19.25
    DRAM Throughput                                 2.11
    Duration                                      60,032
    L1/TEX Cache Throughput                        57.94
    L2 Cache Throughput                            15.28
    SM Active Cycles                           22,153.16
    Compute (SM) Throughput                         2.83
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.95
    Issued Ipc Active                        0.04
    SM Busy                                  5.18
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.82%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        140,247,334,754.80
    Mem Busy                                              12.89
    Max Bandwidth                                         19.25
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           53.18
    Mem Pipes Busy                                         2.83
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.893%                                                                                     
          Out of the 29408864.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 18.05%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.96
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.04
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.75%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 104.1 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         104.47
    Warp Cycles Per Executed Instruction                       104.51
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.24%                                                                                     
          On average, each warp of this workload spends 77.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.2% of the total average of 104.5 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.42
    Executed Instructions                                     125,160
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.24
    Achieved Active Warps Per SM                        3.99
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.74%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.75%                                                                                     
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          4,111
    Total DRAM Elapsed Cycles                      15,335,424
    Average L1 Active Cycles                        22,153.16
    Total L1 Elapsed Cycles                         9,869,618
    Average L2 Active Cycles                        60,192.22
    Total L2 Elapsed Cycles                        12,742,920
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        22,153.16
    Total SM Elapsed Cycles                         9,869,618
    Average SMSP Active Cycles                      22,027.58
    Total SMSP Elapsed Cycles                      39,478,472
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 21.44%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 64.55% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.27%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.40% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 21.44%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.55% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 81.48%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  kernel_cutlass_kernel_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12812864_TVLayoutA1128641281128_TVLayoutB1128641281128_TVLayoutC11281281281128_CopyAtom_ThrI_0 (1, 56, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 10.0
==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ----------------
    Metric Name             Metric Unit     Metric Value
    ----------------------- ----------- ----------------
    DRAM Frequency                      3,991,199,119.91
    SM Frequency                        1,122,900,766.64
    Elapsed Cycles                                66,180
    Memory Throughput                              19.10
    DRAM Throughput                                 2.18
    Duration                                      58,176
    L1/TEX Cache Throughput                        58.72
    L2 Cache Throughput                            15.77
    SM Active Cycles                           21,860.61
    Compute (SM) Throughput                         2.84
    ----------------------- ----------- ----------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The workload achieved 0%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active                      0.04
    Executed Ipc Elapsed                     0.01
    Issue Slots Busy                         0.97
    Issued Ipc Active                        0.04
    SM Busy                                  5.25
    -------------------- ----------- ------------

    OPT   Estimated Speedup: 94.75%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------------
    Metric Name                  Metric Unit       Metric Value
    ---------------------------- ----------- ------------------
    Memory Throughput                        144,721,672,167.22
    Mem Busy                                              12.84
    Max Bandwidth                                         19.10
    L1/TEX Hit Rate                                       98.44
    L2 Compression Success Rate                         (!) n/a
    L2 Compression Input Sectors                        (!) n/a
    L2 Hit Rate                                           52.38
    Mem Pipes Busy                                         2.84
    ---------------------------- ----------- ------------------

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Chart
    OPT   Estimated Speedup: 0.9217%                                                                                    
          Out of the 29408960.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To       
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Memory Workload Analysis Tables
    OPT   Estimated Speedup: 17.91%                                                                                     
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 2.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

==WARNING== Failed to detect compute capability, assuming 50
    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                             0.97
    Issued Warp Per Scheduler                        0.01
    No Eligible                                     99.03
    Active Warps Per Scheduler                       1.00
    Eligible Warps Per Scheduler                     0.01
    ---------------------------- ----------- ------------

    OPT   Estimated Speedup: 80.9%                                                                                      
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 103.3 cycles. This might leave hardware resources underutilized and may lead to   
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

==WARNING== Failed to detect compute capability, assuming 50
    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction                         103.31
    Warp Cycles Per Executed Instruction                       103.35
    Avg. Active Threads Per Warp                                31.62
    Avg. Not Predicated Off Threads Per Warp                    29.86
    ---------------------------------------- ----------- ------------

    OPT   Estimated Speedup: 74.66%                                                                                     
          On average, each warp of this workload spends 77.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 74.7% of the total average of 103.3 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

==WARNING== Failed to detect compute capability, assuming 50
    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler                   211.43
    Executed Instructions                                     125,164
    Avg. Issued Instructions Per Scheduler                     211.51
    Issued Instructions                                       125,216
    ---------------------------------------- ----------- ------------

==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
==WARNING== Failed to detect compute capability, assuming 50
    Section: Launch Statistics
    -------------------------------- ----------- ---------------
    Metric Name                      Metric Unit    Metric Value
    -------------------------------- ----------- ---------------
    Block Size                                               128
    Function Cache Configuration                 CachePreferNone
    Grid Size                                                 56
    Registers Per Thread                                     136
    Shared Memory Configuration Size                     135,168
    Driver Shared Memory Per Block                         1,024
    Dynamic Shared Memory Per Block                       36,992
    Static Shared Memory Per Block                             0
    # SMs                                                    148
    Stack Size                                             1,024
    Threads                                                7,168
    # TPCs                                                    74
    Enabled TPC IDs                                          all
    Uses Green Context                                         0
    Waves Per SM                                            0.04
    -------------------------------- ----------- ---------------

    OPT   Estimated Speedup: 62.16%                                                                                     
          The grid for this launch is configured to execute only 56 blocks, which is less than the GPU's 148            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

==WARNING== Failed to detect compute capability, assuming 50
    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                              18.75
    Achieved Occupancy                                  6.25
    Achieved Active Warps Per SM                        4.00
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 66.67%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 80.9%                                                                                      
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

==WARNING== Failed to detect compute capability, assuming 50
    Section: GPU and Memory Workload Distribution
    -------------------------------- ----------- ------------
    Metric Name                      Metric Unit Metric Value
    -------------------------------- ----------- ------------
    Average DRAM Active Cycles                          4,111
    Total DRAM Elapsed Cycles                      14,860,288
    Average L1 Active Cycles                        21,860.61
    Total L1 Elapsed Cycles                         9,945,976
    Average L2 Active Cycles                        59,678.66
    Total L2 Elapsed Cycles                        12,348,240
    Average MC Channel Active Cycles                  (!) n/a
    Total MC Channel Elapsed Cycles                   (!) n/a
    Average SM Active Cycles                        21,860.61
    Total SM Elapsed Cycles                         9,945,976
    Average SMSP Active Cycles                      21,843.38
    Total SMSP Elapsed Cycles                      39,783,904
    -------------------------------- ----------- ------------

    OPT   Estimated Speedup: 20.81%                                                                                     
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 63.97% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 20.87%                                                                                     
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 64.20% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Estimated Speedup: 20.81%                                                                                     
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.97% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

==WARNING== Failed to detect compute capability, assuming 50
    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio                     0.04
    Branch Instructions                          5,152
    Branch Efficiency                            98.81
    Avg. Divergent Branches                       0.09
    ------------------------- ----------- ------------

    OPT   Estimated Speedup: 83.37%                                                                                     
          This kernel has uncoalesced global accesses resulting in a total of 860160 excessive sectors (94% of the      
          total 917504 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

