NCU AGGREGATED PERFORMANCE SUMMARY (Averaged over all kernels)
================================================================================

RANKED BOTTLENECKS (Highest Potential Speedup First):
Section Name                                       | Avg. Speedup % 
----------------------------------------------------------------------
Compute Workload Analysis                          |         95.43%
Source Counters                                    |         85.11%
Scheduler Statistics                               |         83.49%
Warp State Statistics                              |         76.53%
Occupancy                                          |         73.77%
Launch Statistics                                  |         67.57%
GPU and Memory Workload Distribution               |         19.29%
Memory Workload Analysis Tables                    |         15.47%
Memory Workload Analysis Chart                     |          3.73%

================================================================================

### Section: Compute Workload Analysis
--------------------------------------------- ----------- ------------
Executed Ipc Active                                       0.08        
Executed Ipc Elapsed                                      0.02        
Issue Slots Busy                                          0.57        
Issued Ipc Active                                         0.08        
SM Busy                                                   4.57        

>> GLOBAL AVERAGE ESTIMATED SPEEDUP: 95.43%
================================================================================

### Section: GPU Speed Of Light Throughput
--------------------------------------------- ----------- ------------
Compute (SM) Throughput                                   4.98        
DRAM Frequency                                            3991132931.71
DRAM Throughput                                           5.63        
Duration                                                  71125.33    
Elapsed Cycles                                            81625.00    
L1/TEX Cache Throughput                                   58.75       
L2 Cache Throughput                                       13.11       
Memory Throughput                                         16.51       
SM Active Cycles                                          24101.85    
SM Frequency                                              1132668029.70
================================================================================

### Section: GPU and Memory Workload Distribution
--------------------------------------------- ----------- ------------
Average DRAM Active Cycles                                15140.37    
Average L1 Active Cycles                                  24101.85    
Average L2 Active Cycles                                  75007.13    
Average SM Active Cycles                                  24101.85    
Average SMSP Active Cycles                                24005.93    
Total DRAM Elapsed Cycles                                 18168661.33 
Total L1 Elapsed Cycles                                   12077873.33 
Total L2 Elapsed Cycles                                   15105817.33 
Total SM Elapsed Cycles                                   12077873.33 
Total SMSP Elapsed Cycles                                 48311493.33 

>> GLOBAL AVERAGE ESTIMATED SPEEDUP: 19.29%

Advice & Analysis (Averaged Trends):
 - One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 69.08% above the average, while the minimum instance value is 100.00% below the average.

 - One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 69.09% above the average, while the minimum instance value is 100.00% below the average.

================================================================================

### Section: Instruction Statistics
--------------------------------------------- ----------- ------------
Avg. Executed Instructions Per Scheduler                  504.76      
Avg. Issued Instructions Per Scheduler                    505.54      
Executed Instructions                                     298817.33   
Issued Instructions                                       299280.00   
================================================================================

### Section: Launch Statistics
--------------------------------------------- ----------- ------------
# SMs                                                     148.00      
# TPCs                                                    74.00       
Block Size                                                128.00      
Driver Shared Memory Per Block                            1024.00     
Dynamic Shared Memory Per Block                           73856.00    
Grid Size                                                 48.00       
Preferred Cluster Size                                    0.00        
Registers Per Thread                                      136.00      
Shared Memory Configuration Size                          233472.00   
Stack Size                                                1024.00     
Static Shared Memory Per Block                            0.00        
Threads                                                   6144.00     
Uses Green Context                                        0.00        
Waves Per SM                                              0.11        

>> GLOBAL AVERAGE ESTIMATED SPEEDUP: 67.57%
================================================================================

### Section: Memory Workload Analysis
--------------------------------------------- ----------- ------------
L1/TEX Hit Rate                                           89.30       
L2 Compression Input Sectors                              788039.67   
L2 Compression Ratio                                      0.00        
L2 Compression Success Rate                               0.00        
L2 Hit Rate                                               43.77       
Local Memory Spilling Request Overhead                    0.00        
Local Memory Spilling Requests                            0.00        
Max Bandwidth                                             16.51       
Mem Busy                                                  11.40       
Mem Pipes Busy                                            4.37        
Memory Throughput                                         374530661959.34
================================================================================

### Section: Occupancy
--------------------------------------------- ----------- ------------
Achieved Active Warps Per SM                              3.98        
Achieved Occupancy                                        6.23        
Block Limit Registers                         block       3.00        
Block Limit SM                                block       32.00       
Block Limit Shared Mem                        block       3.00        
Block Limit Warps                             block       16.00       
Theoretical Active Warps per SM               warp        12.00       
Theoretical Occupancy                                     18.75       

>> GLOBAL AVERAGE ESTIMATED SPEEDUP: 73.77%

Advice & Analysis (Averaged Trends):
 - The difference between calculated theoretical (18.80%) and measured achieved occupancy (6.20%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.

================================================================================

### Section: Scheduler Statistics
--------------------------------------------- ----------- ------------
Active Warps Per Scheduler                                1.00        
Eligible Warps Per Scheduler                              0.02        
Issued Warp Per Scheduler                                 0.02        
No Eligible                                               98.00       
One or More Eligible                                      2.00        

>> GLOBAL AVERAGE ESTIMATED SPEEDUP: 83.49%
================================================================================

### Section: Source Counters
--------------------------------------------- ----------- ------------
Avg. Divergent Branches                                   0.08        
Branch Efficiency                                         99.46       
Branch Instructions                                       10498.00    
Branch Instructions Ratio                                 0.04        

>> GLOBAL AVERAGE ESTIMATED SPEEDUP: 85.11%
================================================================================

### Section: Warp State Statistics
--------------------------------------------- ----------- ------------
Avg. Active Threads Per Warp                              31.83       
Avg. Not Predicated Off Threads Per Warp                  30.60       
Warp Cycles Per Executed Instruction                      54.52       
Warp Cycles Per Issued Instruction                        54.46       

>> GLOBAL AVERAGE ESTIMATED SPEEDUP: 76.53%

Advice & Analysis (Averaged Trends):
 - On average, each warp of this workload spends 41.27 cycles being stalled waiting for a scoreboard dependency on a L1.00TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1.00TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 76.53% of the total average of 54.47 cycles between issuing two instructions.

================================================================================

